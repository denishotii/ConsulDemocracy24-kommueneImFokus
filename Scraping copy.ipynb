{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper for 19 Cities\n",
    "\n",
    "This scraper extracts and organizes data into three main DataFrames:\n",
    "1. **`all_projects_df`**: Contains all projects from the websites.\n",
    "   - Columns: `Project URL`, `Project Title`, `Project Description`, `Proposal Count`, `City`\n",
    "\n",
    "2. **`all_proposals_df`**: Contains all proposals under projects.\n",
    "   - Columns: `URL`, `Title`, `Proposed for Project`, `Description`, `Author`, `Comments`, `Supporters`, `City`\n",
    "\n",
    "3. **`all_comments_df`**: Contains all comments under projects and proposals.\n",
    "   - Columns: `URL`, `Project`, `Text`, `Author`, `Likes`, `Dislikes`, `Date`, `City`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping project at https://flensburg-mitmachen.dehttps://survey.lamapoll.de/Publikumspreis-Kommune-bewegt-Welt-2024: HTTPSConnectionPool(host='flensburg-mitmachen.dehttps', port=443): Max retries exceeded with url: /survey.lamapoll.de/Publikumspreis-Kommune-bewegt-Welt-2024 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000018D29CE5AB0>: Failed to resolve 'flensburg-mitmachen.dehttps' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Updated function to extract proposals from a project page\n",
    "def extract_proposals(soup, base_url):\n",
    "    proposals = []\n",
    "    proposal_items = soup.find_all('div', class_='resource-item proposal-list-item')\n",
    "\n",
    "    for proposal in proposal_items:\n",
    "        # Extract title\n",
    "        title_tag = proposal.find('a', class_='resource-item--title')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "        # Extract URL\n",
    "        url = base_url + title_tag['href'] if title_tag and 'href' in title_tag.attrs else None\n",
    "\n",
    "        # Extract description\n",
    "        description_tag = proposal.find('div', class_='resource-item--description')\n",
    "        description = description_tag.get_text(strip=True) if description_tag else None\n",
    "\n",
    "        # Extract author/username\n",
    "        author_tag = proposal.find('a', class_='resource-item--author')\n",
    "        author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "        # Extract number of comments\n",
    "        comments_tag = proposal.find('span', class_='comments')\n",
    "        comments = int(comments_tag.get_text(strip=True).split()[0]) if comments_tag else 0\n",
    "\n",
    "        # Extract number of supporters\n",
    "        supporters_tag = proposal.find('span', class_='total-supports')\n",
    "        supporters = int(supporters_tag.get_text(strip=True).split()[0]) if supporters_tag else 0\n",
    "\n",
    "        # Extract parent project\n",
    "        project_tag = proposal.find('a', class_='breadcrumbs-item')\n",
    "        proposed_for_project = project_tag.get_text(strip=True) if project_tag else None\n",
    "\n",
    "        proposals.append({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Proposed for Project': proposed_for_project,\n",
    "            'Description': description,\n",
    "            'Author': author,\n",
    "            'Comments': comments,\n",
    "            'Supporters': supporters,\n",
    "        })\n",
    "    return proposals\n",
    "\n",
    "\n",
    "# Function to extract city name from the base URL\n",
    "def extract_city_name(base_url):\n",
    "    # Words to remove from the city name\n",
    "    remove_words = ['mitmachen', 'Mitmachen', 'mitwirken', 'Smarte', 'region', 'unser', 'mitgestalten', 'gestalten', 'machmit', 'dialog', 'consul', 'www', 'de', 'https', 'com']\n",
    "\n",
    "    # Split the URL into parts (by '.' or '/')\n",
    "    parts = base_url.replace('https://', '').replace('http://', '').split('.')\n",
    "    all_parts = [part.split('/')[0] for part in parts]  # Handle cases where \"/\" exists after domain\n",
    "\n",
    "    # Remove known unwanted words and empty strings\n",
    "    filtered_parts = [part for part in all_parts if part.lower() not in remove_words and part]\n",
    "\n",
    "    # Return the first relevant part (assumes city name is left after filtering)\n",
    "    city = filtered_parts[0].replace('-', ' ').capitalize() if filtered_parts else \"Unknown\"\n",
    "\n",
    "    # Remove unwanted words from city name\n",
    "    for word in remove_words:\n",
    "        city = city.replace(word, '')\n",
    "\n",
    "    return city.strip().capitalize()\n",
    "\n",
    "\n",
    "# Update project scraping to exclude comments\n",
    "def scrape_project_page_with_proposals(url, base_url, city):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load project page: {url}\")\n",
    "        return None, []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract project title\n",
    "    title_tag = soup.find('title')\n",
    "    project_title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "    # Extract project description\n",
    "    content_div = soup.find('div', class_='flex-layout')\n",
    "    description = content_div.get_text(strip=True) if content_div else None\n",
    "\n",
    "    # Extract proposals\n",
    "    proposals = extract_proposals(soup, base_url=base_url)\n",
    "\n",
    "    return {\n",
    "        'Project URL': url,\n",
    "        'Project Title': project_title,\n",
    "        'Project Description': description,\n",
    "        'Proposal Count': len(proposals),\n",
    "    }, proposals\n",
    "\n",
    "\n",
    "# Modified function to scrape projects with proposals\n",
    "def scrape_projects_with_proposals(main_url, base_url):\n",
    "    response = requests.get(main_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load main projects page: {main_url}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all project links\n",
    "    links = soup.find_all('a', class_='resource-item--title')\n",
    "    project_links = [base_url + link['href'] for link in links if 'href' in link.attrs]\n",
    "\n",
    "    projects = []\n",
    "    all_proposals = []\n",
    "\n",
    "    for project_url in project_links:\n",
    "        try:\n",
    "            project_data, proposals = scrape_project_page_with_proposals(\n",
    "                project_url, base_url, extract_city_name(base_url)\n",
    "            )\n",
    "            if project_data:\n",
    "                projects.append(project_data)\n",
    "                all_proposals.extend(proposals)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping project at {project_url}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(projects), pd.DataFrame(all_proposals)\n",
    "\n",
    "# List of websites (fixed flensburg-mitmachen.de base_url)\n",
    "websites = [\n",
    "    {\"main_url\": \"https://wuerzburg-mitmachen.de/projekts\", \"base_url\": \"https://wuerzburg-mitmachen.de\"},\n",
    "    {\"main_url\": \"https://mitmachen.siegburg.de/projekts\", \"base_url\": \"https://mitmachen.siegburg.de\"}, \n",
    "    {\"main_url\": \"https://mitmachen.jena.de/projekts\", \"base_url\": \"https://mitmachen.jena.de\"},\n",
    "    {\"main_url\": \"https://mitmachgemeinde.de/projekts\", \"base_url\": \"https://mitmachgemeinde.de\"},\n",
    "    {\"main_url\": \"https://bamberg-gestalten.de/projekts\", \"base_url\": \"https://bamberg-gestalten.de\"},\n",
    "    {\"main_url\": \"https://mitmachen-pforzheim.de/projekts\", \"base_url\": \"https://mitmachen-pforzheim.de\"},\n",
    "    {\"main_url\": \"https://bochum-mitgestalten.de/projekts\", \"base_url\": \"https://bochum-mitgestalten.de\"},\n",
    "    {\"main_url\": \"https://unser.muenchen.de/projekts\", \"base_url\": \"https://unser.muenchen.de\"},\n",
    "    {\"main_url\": \"https://mitreden.ilzerland.bayern/projekts\", \"base_url\": \"https://mitreden.ilzerland.bayern\"},\n",
    "    {\"main_url\": \"https://stutensee-mitwirken.de/projekts\", \"base_url\": \"https://stutensee-mitwirken.de\"},\n",
    "    {\"main_url\": \"https://consul.unterschleissheim.de/projekts\", \"base_url\": \"https://consul.unterschleissheim.de\"},\n",
    "    {\"main_url\": \"https://machmit.kempten.de/projekts\", \"base_url\": \"https://machmit.kempten.de\"},\n",
    "    {\"main_url\": \"https://consul.detmold-mitgestalten.de/projekts\", \"base_url\": \"https://consul.detmold-mitgestalten.de\"},\n",
    "    {\"main_url\": \"https://flensburg-mitmachen.de/projekts\", \"base_url\": \"https://flensburg-mitmachen.de\"},  # Fixed URL\n",
    "    {\"main_url\": \"https://mitmachen.amberg.de/projekts\", \"base_url\": \"https://mitmachen.amberg.de\"},\n",
    "    {\"main_url\": \"https://mitmachen.smarte-region-linz.de/projekts\", \"base_url\": \"https://mitmachen.smarte-region-linz.de\"},\n",
    "    {\"main_url\": \"https://mitgestalten.trier.de/projekts\", \"base_url\": \"https://mitgestalten.trier.de\"},\n",
    "    {\"main_url\": \"https://machmit.augsburg.de/projekts\", \"base_url\": \"https://machmit.augsburg.de\"}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize empty DataFrames for all projects and proposals\n",
    "all_projects_df = pd.DataFrame()\n",
    "all_proposals_df = pd.DataFrame()\n",
    "\n",
    "# Main loop to scrape all websites\n",
    "for site in websites:\n",
    "    main_url = site[\"main_url\"]\n",
    "    base_url = site[\"base_url\"]\n",
    "\n",
    "    city = extract_city_name(base_url)\n",
    "\n",
    "    try:\n",
    "        # Scrape projects and proposals\n",
    "        projects_df, proposals_df = scrape_projects_with_proposals(main_url, base_url)\n",
    "\n",
    "        # Add a 'City' column to all DataFrames\n",
    "        projects_df['City'] = city\n",
    "        proposals_df['City'] = city\n",
    "\n",
    "        # Append results to the combined DataFrames\n",
    "        all_projects_df = pd.concat([all_projects_df, projects_df], ignore_index=True)\n",
    "        all_proposals_df = pd.concat([all_proposals_df, proposals_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {main_url} - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Proposed for Project</th>\n",
       "      <th>Description</th>\n",
       "      <th>Author</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Supporters</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/110-autofreier-bischofshut</td>\n",
       "      <td>Autofreier Bischofshut</td>\n",
       "      <td>Zukunftskonzepte für die Innenstadt</td>\n",
       "      <td>Wir fordern die Ausrufung des Klimanotstands, damit Belange unseres Klimas vor das wirtschaftlic...</td>\n",
       "      <td>Letzte Generation Würzburg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/109-e-scooter-verbieten</td>\n",
       "      <td>E Scooter verbieten</td>\n",
       "      <td>Zukunftskonzepte für die Innenstadt</td>\n",
       "      <td>E Scooter sollten (im Innenstadtbereich) verboten werden. Diese werden häufig willkürlich abgest...</td>\n",
       "      <td>Ccmuet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/108-barrierefrei-ins-nautiland-lgs</td>\n",
       "      <td>Barrierefrei ins Nautiland/LGS</td>\n",
       "      <td>Zukunftskonzepte für die Innenstadt</td>\n",
       "      <td>Nautiland - neu.\\r\\nUmweltstation - neu.\\r\\nZellertorauffahrt - neu.\\r\\nLeider fehlen die barrie...</td>\n",
       "      <td>AASeuffert</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/107-kinderabenteuer-indoor-spielplatz-smaland</td>\n",
       "      <td>Kinderabenteuer / Indoor Spielplatz / Smaland</td>\n",
       "      <td>Zukunftskonzepte für die Innenstadt</td>\n",
       "      <td>Es gibt zwar schon den FunPark für Kinder mit Trampolinhalle etc. in der Nähe der Nürnberger Str...</td>\n",
       "      <td>ABlitz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/106-banke-und-grun-im-neu-gestalteten-bereich-karmelite...</td>\n",
       "      <td>Bänke und \"Grün\" im neu gestalteten Bereich Karmelitenstraße/Vierröhr...</td>\n",
       "      <td>Zukunftskonzepte für die Innenstadt</td>\n",
       "      <td>Die Baustelle von der Karmelitenstraße zum Vierröhrenbrunnen wurde vor kurzem abgeschlossen. Die...</td>\n",
       "      <td>Ccmuet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   URL  \\\n",
       "0                                  https://wuerzburg-mitmachen.de/proposals/110-autofreier-bischofshut   \n",
       "1                                     https://wuerzburg-mitmachen.de/proposals/109-e-scooter-verbieten   \n",
       "2                          https://wuerzburg-mitmachen.de/proposals/108-barrierefrei-ins-nautiland-lgs   \n",
       "3               https://wuerzburg-mitmachen.de/proposals/107-kinderabenteuer-indoor-spielplatz-smaland   \n",
       "4  https://wuerzburg-mitmachen.de/proposals/106-banke-und-grun-im-neu-gestalteten-bereich-karmelite...   \n",
       "\n",
       "                                                                      Title  \\\n",
       "0                                                    Autofreier Bischofshut   \n",
       "1                                                       E Scooter verbieten   \n",
       "2                                            Barrierefrei ins Nautiland/LGS   \n",
       "3                             Kinderabenteuer / Indoor Spielplatz / Smaland   \n",
       "4  Bänke und \"Grün\" im neu gestalteten Bereich Karmelitenstraße/Vierröhr...   \n",
       "\n",
       "                  Proposed for Project  \\\n",
       "0  Zukunftskonzepte für die Innenstadt   \n",
       "1  Zukunftskonzepte für die Innenstadt   \n",
       "2  Zukunftskonzepte für die Innenstadt   \n",
       "3  Zukunftskonzepte für die Innenstadt   \n",
       "4  Zukunftskonzepte für die Innenstadt   \n",
       "\n",
       "                                                                                           Description  \\\n",
       "0  Wir fordern die Ausrufung des Klimanotstands, damit Belange unseres Klimas vor das wirtschaftlic...   \n",
       "1  E Scooter sollten (im Innenstadtbereich) verboten werden. Diese werden häufig willkürlich abgest...   \n",
       "2  Nautiland - neu.\\r\\nUmweltstation - neu.\\r\\nZellertorauffahrt - neu.\\r\\nLeider fehlen die barrie...   \n",
       "3  Es gibt zwar schon den FunPark für Kinder mit Trampolinhalle etc. in der Nähe der Nürnberger Str...   \n",
       "4  Die Baustelle von der Karmelitenstraße zum Vierröhrenbrunnen wurde vor kurzem abgeschlossen. Die...   \n",
       "\n",
       "                       Author  Comments  Supporters       City  \n",
       "0  Letzte Generation Würzburg       0.0        20.0  Wuerzburg  \n",
       "1                      Ccmuet       0.0         2.0  Wuerzburg  \n",
       "2                  AASeuffert       0.0         2.0  Wuerzburg  \n",
       "3                      ABlitz       0.0         0.0  Wuerzburg  \n",
       "4                      Ccmuet       0.0        12.0  Wuerzburg  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_proposals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project URL</th>\n",
       "      <th>Project Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Proposal Count</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/grombuehl-zukunftssicher</td>\n",
       "      <td>Energetisches Quartierskonzept für Grombühl</td>\n",
       "      <td>Grombühl 2040 - ein SzenarioDie Straßen Grombühls sind grüner, ruhiger und voller Leben. Das „Qu...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/mobilitaetsplan</td>\n",
       "      <td>Mobilitätsplan 2040</td>\n",
       "      <td>Mobilitätsplan 2040 für die Stadt Würzburg: Jetzt mitmachen!Aktuell erstellt die Stadt Würzburg ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftsregion</td>\n",
       "      <td>Zukunftsregion Würzburg</td>\n",
       "      <td>Zukunftsregion Würzburg: Jetzt aktiv mitgestalten!Die Stadt und der Landkreis Würzburg wollen ih...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftskonzepte-fuer-die-innenstadt</td>\n",
       "      <td>Zukunftskonzepte für die Innenstadt</td>\n",
       "      <td>Wie soll die Würzburger Innenstadt von morgen aussehen? Was wünschen sich Bürger:innen, Einzelhä...</td>\n",
       "      <td>24</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/klimaanpassung</td>\n",
       "      <td>Klimaanpassung</td>\n",
       "      <td>Klimaanpassungsstrategie für die Stadt Würzburg: Jetzt mitmachen!Würzburg - Seit Anfang 2024 era...</td>\n",
       "      <td>14</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Project URL  \\\n",
       "0              https://wuerzburg-mitmachen.de/grombuehl-zukunftssicher   \n",
       "1                       https://wuerzburg-mitmachen.de/mobilitaetsplan   \n",
       "2                        https://wuerzburg-mitmachen.de/zukunftsregion   \n",
       "3  https://wuerzburg-mitmachen.de/zukunftskonzepte-fuer-die-innenstadt   \n",
       "4                        https://wuerzburg-mitmachen.de/klimaanpassung   \n",
       "\n",
       "                                 Project Title  \\\n",
       "0  Energetisches Quartierskonzept für Grombühl   \n",
       "1                          Mobilitätsplan 2040   \n",
       "2                      Zukunftsregion Würzburg   \n",
       "3          Zukunftskonzepte für die Innenstadt   \n",
       "4                               Klimaanpassung   \n",
       "\n",
       "                                                                                   Project Description  \\\n",
       "0  Grombühl 2040 - ein SzenarioDie Straßen Grombühls sind grüner, ruhiger und voller Leben. Das „Qu...   \n",
       "1  Mobilitätsplan 2040 für die Stadt Würzburg: Jetzt mitmachen!Aktuell erstellt die Stadt Würzburg ...   \n",
       "2  Zukunftsregion Würzburg: Jetzt aktiv mitgestalten!Die Stadt und der Landkreis Würzburg wollen ih...   \n",
       "3  Wie soll die Würzburger Innenstadt von morgen aussehen? Was wünschen sich Bürger:innen, Einzelhä...   \n",
       "4  Klimaanpassungsstrategie für die Stadt Würzburg: Jetzt mitmachen!Würzburg - Seit Anfang 2024 era...   \n",
       "\n",
       "   Proposal Count       City  \n",
       "0               0  Wuerzburg  \n",
       "1               0  Wuerzburg  \n",
       "2               0  Wuerzburg  \n",
       "3              24  Wuerzburg  \n",
       "4              14  Wuerzburg  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_projects_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BurgerBudgets in Jena (2024, 23, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URLs for the budgets\n",
    "# budget_urls = {\n",
    "#     2024: \"https://mitmachen.jena.de/buergerbudget\",\n",
    "#     2023: \"https://mitmachen.jena.de/buergerbudget-2023\",\n",
    "#     2022: \"https://mitmachen.jena.de/buergerbudget-2022\"\n",
    "# }\n",
    "\n",
    "# # Updated function to scrape and clean a budget table for a given year\n",
    "# def scrape_and_clean_budget_table(url, year):\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to load URL: {url}\")\n",
    "#         return None\n",
    "    \n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     table = soup.find('table', id='budget-investments-compatible')  # Locate the table by its ID\n",
    "    \n",
    "#     if not table:\n",
    "#         print(f\"No table found for URL: {url}\")\n",
    "#         return None\n",
    "    \n",
    "#     # Extract the total available budget for the year (last <th> in <thead>)\n",
    "#     available_budget_tag = table.find('thead').find_all('th')[-1]  # Find the last <th>\n",
    "#     available_budget = (\n",
    "#         float(re.sub(r'[^\\d.]', '', available_budget_tag.get_text(strip=True))) * 1000\n",
    "#         if available_budget_tag else None\n",
    "#     )\n",
    "    \n",
    "#     # Extract table headers\n",
    "#     headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')]\n",
    "    \n",
    "#     # Extract table rows\n",
    "#     rows = []\n",
    "#     for tr in table.find('tbody').find_all('tr'):\n",
    "#         # Extract row cells\n",
    "#         cells = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "        \n",
    "#         # Check the class of the <tr> tag for \"success\" or \"discarded\"\n",
    "#         approved = 1 if 'success' in tr.get('class', []) else 0\n",
    "        \n",
    "#         # Append cells and approval status\n",
    "#         rows.append(cells + [approved])\n",
    "    \n",
    "#     # Add \"Approved\" column to the headers\n",
    "#     headers.append('Approved')\n",
    "    \n",
    "#     # Create a DataFrame\n",
    "#     df = pd.DataFrame(rows, columns=headers)\n",
    "#     df['Year'] = year  # Add a 'Year' column\n",
    "#     df['Available Budget'] = available_budget  # Add the total budget for the year to every row\n",
    "#     return df\n",
    "\n",
    "# # Scrape and clean tables for all years\n",
    "# budget_dataframes = [\n",
    "#     scrape_and_clean_budget_table(url, year) for year, url in budget_urls.items()\n",
    "# ]\n",
    "\n",
    "# # Combine all dataframes into one\n",
    "# budget_jena_df = pd.concat(budget_dataframes, ignore_index=True)\n",
    "\n",
    "# # Clean and transform the DataFrame\n",
    "# budget_jena_df['Preis'] = budget_jena_df['Preis'].str.extract(r'(\\d+)').astype(float) * 1000\n",
    "# budget_jena_df['Stimmen'] = budget_jena_df['Stimmen'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# # Rename columns to English\n",
    "# budget_jena_df.rename(columns={\n",
    "#     'Vorschlag Titel': 'Proposal Title',\n",
    "#     'Stimmen': 'Votes',\n",
    "#     'Preis': 'Price',\n",
    "#     'Year': 'Year',\n",
    "#     'Available Budget': 'Budget for this year',\n",
    "#     'Approved': 'Approved'\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Drop unnecessary columns if any remain\n",
    "# budget_jena_df = budget_jena_df.loc[:, ~budget_jena_df.columns.str.contains('VerfügbareBudgetmittel', na=False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments Scraper\n",
    "\n",
    "This scraper extracts comments for all projects from the `all_projects_df` DataFrame and organizes them into a structured DataFrame:\n",
    "\n",
    "1. **`df_comments`**: Contains all comments associated with projects.\n",
    "   - Columns:\n",
    "     - `URL`: The URL of the project the comment is associated with.\n",
    "     - `Project`: The title of the project the comment is associated with.\n",
    "     - `City`: The city the project belongs to (extracted from the URL).\n",
    "     - `Text`: The content of the comment.\n",
    "     - `Username`: The name of the user who posted the comment.\n",
    "     - `Date`: The date the comment was posted.\n",
    "     - `Likes`: The number of likes the comment received.\n",
    "     - `Dislikes`: The number of dislikes the comment received.\n",
    "     - `Total Votes`: The total votes (likes + dislikes) the comment received.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good scraper for comments (748 entries)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Updated function to extract comments from a single page\n",
    "def extract_comments_from_page(soup):\n",
    "    comments_data = []\n",
    "    comments_section = soup.find_all('div', class_='comment small-12')\n",
    "    \n",
    "    for comment in comments_section:\n",
    "        # Extract comment text\n",
    "        comment_text = comment.find('p').get_text(strip=True) if comment.find('p') else None\n",
    "        \n",
    "        # Extract username\n",
    "        username_tag = comment.find('span', class_='user-name')\n",
    "        username = username_tag.get_text(strip=True) if username_tag else None\n",
    "        \n",
    "        # Extract date\n",
    "        date_tag = comment.find('div', class_='comment-info').find_all('a')[-1]\n",
    "        date = date_tag.get_text(strip=True) if date_tag else None\n",
    "\n",
    "        \n",
    "        # Extract likes and dislikes (clean and convert to integer)\n",
    "        likes_tag = comment.find('span', class_='in-favor')\n",
    "        likes = int(re.sub(r'\\D', '', likes_tag.get_text(strip=True))) if likes_tag else 0\n",
    "        \n",
    "        dislikes_tag = comment.find('span', class_='against')\n",
    "        dislikes = int(re.sub(r'\\D', '', dislikes_tag.get_text(strip=True))) if dislikes_tag else 0\n",
    "        \n",
    "        # Extract total votes (clean and convert to integer)\n",
    "        total_votes = likes + dislikes\n",
    "        \n",
    "        comments_data.append({\n",
    "            'Text': comment_text,\n",
    "            'Username': username,\n",
    "            'Date': date,\n",
    "            'Likes': likes,\n",
    "            'Dislikes': dislikes,\n",
    "            'Total Votes': total_votes\n",
    "        })\n",
    "    return comments_data\n",
    "\n",
    "\n",
    "# Function to extract city name from the base URL\n",
    "def extract_city_name(base_url):\n",
    "    # Words to remove from the city name\n",
    "    remove_words = ['mitmachen', 'Mitmachen', 'mitwirken', 'Smarte', 'region', 'unser', 'mitgestalten', 'gestalten', 'machmit', 'dialog', 'consul', 'www', 'de', 'https', 'com']\n",
    "\n",
    "    # Split the URL into parts (by '.' or '/')\n",
    "    parts = base_url.replace('https://', '').replace('http://', '').split('.')\n",
    "    all_parts = [part.split('/')[0] for part in parts]  # Handle cases where \"/\" exists after domain\n",
    "\n",
    "    # Remove known unwanted words and empty strings\n",
    "    filtered_parts = [part for part in all_parts if part.lower() not in remove_words and part]\n",
    "\n",
    "    # Return the first relevant part (assumes city name is left after filtering)\n",
    "    city = filtered_parts[0].replace('-', ' ').capitalize() if filtered_parts else \"Unknown\"\n",
    "\n",
    "    # Remove unwanted words from city name\n",
    "    for word in remove_words:\n",
    "        city = city.replace(word, '')\n",
    "    \n",
    "\n",
    "    return city.strip().capitalize()\n",
    "\n",
    "\n",
    "# Scrape all comments across pages (pagination logic remains the same)\n",
    "def scrape_all_comments(base_url):\n",
    "    comments = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        paginated_url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        response = requests.get(paginated_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load page {page} for URL: {base_url}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        comments_on_page = extract_comments_from_page(soup)\n",
    "        \n",
    "        if not comments_on_page:  # Stop if no comments on the page\n",
    "            break\n",
    "        \n",
    "        comments.extend(comments_on_page)\n",
    "        page += 1\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Function to scrape the main content and comments for each URL\n",
    "def scrape_content_and_comments(urls):\n",
    "    data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load URL: {url}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Scrape main content\n",
    "        title = soup.find('title').get_text(strip=True) if soup.find('title') else None\n",
    "        content_div = soup.find('div', class_='flex-layout')\n",
    "        content = content_div.get_text(strip=True) if content_div else None\n",
    "        \n",
    "        # Scrape comments\n",
    "        comments = scrape_all_comments(url)\n",
    "        \n",
    "        data.append({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Content': content,\n",
    "            'Comments': comments\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Scrape comments for all project URLs from all_projects_df\n",
    "urls = all_projects_df['Project URL'].tolist()  # Use the 'Project URL' column from all_projects_df\n",
    "scraped_data = scrape_content_and_comments(urls)\n",
    "\n",
    "# Create structured DataFrame for comments\n",
    "comments_data = []\n",
    "for item in scraped_data:\n",
    "    for comment in item['Comments']:\n",
    "        comment['URL'] = item['URL']  # Link comment to the project URL\n",
    "        # Extract city name from URL\n",
    "        city = extract_city_name(item['URL'])\n",
    "        comment['City'] = city\n",
    "        comments_data.append(comment)\n",
    "\n",
    "# Create the comments DataFrame\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Create a mapping from URL to Project Title\n",
    "url_to_title = all_projects_df.set_index('Project URL')['Project Title'].to_dict()\n",
    "\n",
    "# Add a 'Project' column to df_comments using the mapping\n",
    "df_comments['Project'] = df_comments['URL'].map(url_to_title)\n",
    "df_comments = df_comments[['URL', 'Project', 'City'] + [col for col in df_comments.columns if col not in ['URL', 'Project', 'City']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Username\n",
       "Lars Löw           34\n",
       "klaus.kleiner77    32\n",
       "Der,wo             31\n",
       "PM                 18\n",
       "Klaus.kleiner77    16\n",
       "                   ..\n",
       "Julius Kuhn         1\n",
       "Juliane88           1\n",
       "Juliane Fuchs       1\n",
       "Julian Sing         1\n",
       "🐙                   1\n",
       "Name: count, Length: 387, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cleaaning and structuring for Sieburg (review if it's needed) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Enhanced function to extract all logical parts, including \"Unterstützer*innen\"\n",
    "# def extract_full_data_with_supporters(content):\n",
    "#     # Extract title (everything before the first date)\n",
    "#     title_match = re.search(r'^(.*?)(\\r|\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "#     title = title_match.group(1).strip() if title_match else None\n",
    "\n",
    "#     # Extract date\n",
    "#     date_match = re.search(r'\\d{1,2}\\.\\s\\w+\\s\\d{4}', content)\n",
    "#     date = date_match.group(0) if date_match else None\n",
    "\n",
    "#     # Extract comments count\n",
    "#     comments_match = re.search(r'(\\d+)\\sKommentare', content)\n",
    "#     comments = int(comments_match.group(1)) if comments_match else 0\n",
    "\n",
    "#     # Extract tags (sections with numbers or + signs)\n",
    "#     tags_match = re.findall(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "#     tags = ', '.join(tags_match) if tags_match else None\n",
    "\n",
    "#     # Extract description (everything after \"Geselliges Beisammensein\" or similar patterns)\n",
    "#     description_start = re.search(r'(Geselliges Beisammensein|Angebotslandkarte)', content)\n",
    "#     description = content[description_start.start():].strip() if description_start else None\n",
    "\n",
    "#     # Extract username\n",
    "#     username_match = re.search(r'(\\w+\\s\\w+|Beigetreten am:.*?\\d{4})', content)\n",
    "#     username = username_match.group(1).split('Beigetreten am:')[0].strip() if username_match else None\n",
    "\n",
    "#     # Extract Vorschläge count\n",
    "#     vorschlaege_match = re.search(r'Vorschläge(\\d+)', content)\n",
    "#     vorschlaege = int(vorschlaege_match.group(1)) if vorschlaege_match else 0\n",
    "\n",
    "#     # Extract Konto verification status\n",
    "#     konto_match = re.search(r'(Konto\\s(verifiziert|ist nicht verifiziert))', content)\n",
    "#     konto_status = konto_match.group(2) if konto_match else None\n",
    "\n",
    "#     # # Extract registration date\n",
    "#     # registration_match = re.search(r'Beigetreten am:\\s(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "#     # registration_date = registration_match.group(1) if registration_match else None\n",
    "\n",
    "#     # Extract number of Unterstützer*innen\n",
    "#     supporters_match = re.search(r'(\\d+)\\sUnterstützer\\*in', content)\n",
    "#     supporters = int(supporters_match.group(1)) if supporters_match else 0\n",
    "\n",
    "#     return title, date, comments, tags, description, username, vorschlaege, konto_status, supporters\n",
    "\n",
    "# # Apply the enhanced function to the DataFrame and create new columns\n",
    "# df_sieburg[['Title', 'Date', 'Comments', 'Tags', 'Description', 'Username', 'Vorschläge', 'Konto Status', 'Supporters']] = df_sieburg['Content'].apply(\n",
    "#     lambda x: pd.Series(extract_full_data_with_supporters(x))\n",
    "# )\n",
    "\n",
    "\n",
    "# # Function to clean description considering keywords, numeric patterns, and refined starting logic\n",
    "# def clean_description_advanced(content):\n",
    "#     # Define keywords that mark the beginning of the description\n",
    "#     keywords = [\n",
    "#         'Geselliges Beisammensein', 'Natur', 'Hilfe & Beratung', 'Bildung', \n",
    "#         'Musik', 'Bewegung', 'Glaube', 'Kulinarisches', 'Kunst & Kultur', 'Sonstiges',\n",
    "#     ]\n",
    "    \n",
    "#     # Check for keywords first\n",
    "#     for keyword in keywords:\n",
    "#         if keyword in content:\n",
    "#             start_idx = content.find(keyword) + len(keyword)\n",
    "#             description = content[start_idx:].strip()\n",
    "#             description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#             return description\n",
    "\n",
    "#     # If no keyword is found, check for numeric patterns like \"18-24, 25-49, etc.\"\n",
    "#     numeric_pattern = re.search(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "#     if numeric_pattern:\n",
    "#         start_idx = numeric_pattern.end()\n",
    "#         description = content[start_idx:].strip()\n",
    "#         description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#         return description\n",
    "\n",
    "#     # As a fallback, find the first capital letter, quote, or digit to mark the start\n",
    "#     fallback_match = re.search(r'[A-Z\"0-9]', content)\n",
    "#     if fallback_match:\n",
    "#         start_idx = fallback_match.start()\n",
    "#         description = content[start_idx:].strip()\n",
    "#         description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#         return description\n",
    "\n",
    "#     # If nothing works, return the content as is\n",
    "#     return content\n",
    "\n",
    "# # Apply the advanced cleaning function to the Description column\n",
    "# df_sieburg['Description'] = df_sieburg['Content'].apply(clean_description_advanced)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
