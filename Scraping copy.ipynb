{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper for 19 Cities\n",
    "\n",
    "This scraper extracts and organizes data into three main DataFrames:\n",
    "1. **`all_projects_df`**: Contains all projects from the websites.\n",
    "   - Columns: `Project URL`, `Project Title`, `Project Description`, `Proposal Count`, `City`\n",
    "\n",
    "2. **`all_proposals_df`**: Contains all proposals under projects.\n",
    "   - Columns: `URL`, `Title`, `Proposed for Project`, `Description`, `Author`, `Comments`, `Supporters`, `City`\n",
    "\n",
    "3. **`all_comments_df`**: Contains all comments under projects and proposals.\n",
    "   - Columns: `URL`, `Project`, `Text`, `Author`, `Likes`, `Dislikes`, `Date`, `City`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# # Updated function to extract proposals from a project page\n",
    "# def extract_proposals(soup, base_url):\n",
    "#     proposals = []\n",
    "#     proposal_items = soup.find_all('div', class_='resource-item proposal-list-item')\n",
    "#     # here we are looking for all the proposals in the page with the class 'resource-item proposal-list-item'\n",
    "\n",
    "#     # For each proposal item, we loop through and extract the relevant information\n",
    "#     for proposal in proposal_items:\n",
    "#         # Extract title by finding the class 'resource-item--title' and getting the text\n",
    "#         title_tag = proposal.find('a', class_='resource-item--title')\n",
    "#         title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "#         # Extract URL by finding the class 'resource-item--title' and getting the 'href' attribute (because title is a link)\n",
    "#         url = base_url + title_tag['href'] if title_tag and 'href' in title_tag.attrs else None\n",
    "\n",
    "#         # Extract description by finding the class 'resource-item--description' and getting the text\n",
    "#         description_tag = proposal.find('div', class_='resource-item--description')\n",
    "#         description = description_tag.get_text(strip=True) if description_tag else None\n",
    "\n",
    "#         # Extract author/username by finding the class 'resource-item--author' and getting the text\n",
    "#         author_tag = proposal.find('a', class_='resource-item--author')\n",
    "#         author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "#         # Extract number of comments by finding the class 'comments' and getting the text (span is almost like a div, but inline)\n",
    "#         comments_tag = proposal.find('span', class_='comments')\n",
    "#         comments = int(comments_tag.get_text(strip=True).split()[0]) if comments_tag else 0\n",
    "\n",
    "#         # Extract number of supporters by finding the class 'total-supports' and getting the text\n",
    "#         supporters_tag = proposal.find('span', class_='total-supports')\n",
    "#         supporters = int(supporters_tag.get_text(strip=True).split()[0]) if supporters_tag else 0\n",
    "\n",
    "#         # Extract parent project name by finding the class 'breadcrumbs-item' and getting the text\n",
    "#         project_tag = proposal.find('a', class_='breadcrumbs-item')\n",
    "#         proposed_for_project = project_tag.get_text(strip=True) if project_tag else None\n",
    "\n",
    "#         # Append the extracted information to the proposals list. Proposals are dictionaries with keys and values\n",
    "#         proposals.append({\n",
    "#             'URL': url,\n",
    "#             'Title': title,\n",
    "#             'Proposed for Project': proposed_for_project,\n",
    "#             'Description': description,\n",
    "#             'Author': author,\n",
    "#             'Comments': comments,\n",
    "#             'Supporters': supporters,\n",
    "#         })\n",
    "#     return proposals\n",
    "\n",
    "\n",
    "# # Function to extract city name from the base URL\n",
    "# def extract_city_name(base_url):\n",
    "#     # Words to remove from the city name, because URL is not always clean city name and may contain extra words (e.g. mitmachen)\n",
    "#     remove_words = ['mitmachen', 'Mitmachen', 'mitwirken', 'Smarte', 'region', 'unser', 'mitgestalten', 'gestalten', 'machmit', 'dialog', 'consul', 'www', 'de', 'https', 'com']\n",
    "\n",
    "#     # Split the URL into parts (by '.' or '/'), because city name is usually the first relevant part\n",
    "#     parts = base_url.replace('https://', '').replace('http://', '').split('.')\n",
    "#     all_parts = [part.split('/')[0] for part in parts]  # Handle cases where \"/\" exists after domain\n",
    "\n",
    "#     # Remove known unwanted words and empty strings \n",
    "#     filtered_parts = [part for part in all_parts if part.lower() not in remove_words and part]\n",
    "\n",
    "#     # Return the first relevant part (assumes city name is left after filtering) \n",
    "#     city = filtered_parts[0].replace('-', ' ').capitalize() if filtered_parts else \"Unknown\"\n",
    "\n",
    "#     # Because city name is not always first part, we get it by removing all other words (they are usually similar in all URLs)\n",
    "#     for word in remove_words:\n",
    "#         city = city.replace(word, '')\n",
    "\n",
    "#     return city.strip().capitalize()\n",
    "\n",
    "\n",
    "# # Here is the function that scrapes the project page and extracts the project title, description and proposals \n",
    "# def scrape_project_page_with_proposals(url, base_url, city):\n",
    "\n",
    "#     # In that block we request the page and parse it with BeautifulSoup, getting soup object, which contains all the HTML content from the given URL\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to load project page: {url}\")\n",
    "#         return None, []\n",
    "\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "\n",
    "#     # Extract project title by finding the 'title' tag and getting the text\n",
    "#     title_tag = soup.find('title')\n",
    "#     project_title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "#     # Extract project description by finding the 'div' tag with class 'flex-layout' and getting the text\n",
    "#     content_div = soup.find('div', class_='flex-layout')\n",
    "#     description = content_div.get_text(strip=True) if content_div else None\n",
    "\n",
    "#     # Extract proposals by calling the extract_proposals function, which we defined earlier\n",
    "#     proposals = extract_proposals(soup, base_url=base_url)\n",
    "\n",
    "#     # Return a dictionary with the extracted information and the list of proposals\n",
    "#     return {\n",
    "#         'Project URL': url,\n",
    "#         'Project Title': project_title,\n",
    "#         'Project Description': description,\n",
    "#         'Proposal Count': len(proposals),\n",
    "#     }, proposals\n",
    "\n",
    "\n",
    "# # Here is the function that scrapes the main projects page and extracts the project URLs. This function is \"main\" because it calls the scrape_project_page_with_proposals function, which calls the extract_proposals function.\n",
    "# # So the scheme is: scrape_projects_with_proposals(to get project URLs) -> scrape_project_page_with_proposals(to get project data and proposals) -> extract_proposals(to get proposals)\n",
    "# def scrape_projects_with_proposals(main_url, base_url):\n",
    "\n",
    "#     # Here we again request the page and parse it with BeautifulSoup, getting soup object, which contains all the HTML content from the given URL\n",
    "#     response = requests.get(main_url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to load main projects page: {main_url}\")\n",
    "#         return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     # Find all project links by finding the 'a' tag with class 'resource-item--title' and getting the 'href' attribute\n",
    "#     links = soup.find_all('a', class_='resource-item--title')\n",
    "#     project_links = [base_url + link['href'] for link in links if 'href' in link.attrs]\n",
    "\n",
    "#     projects = []\n",
    "#     all_proposals = []\n",
    "\n",
    "#     # For each project link, call the scrape_project_page_with_proposals function to get the project data and proposals. Try/except block is used to catch any errors that may occur during scraping\n",
    "\n",
    "#     for project_url in project_links:\n",
    "#         try:\n",
    "#             project_data, proposals = scrape_project_page_with_proposals(\n",
    "#                 project_url, base_url, extract_city_name(base_url)\n",
    "#             )\n",
    "#             if project_data:\n",
    "#                 projects.append(project_data)\n",
    "#                 all_proposals.extend(proposals)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error scraping project at {project_url}: {e}\")\n",
    "\n",
    "#     return pd.DataFrame(projects), pd.DataFrame(all_proposals)\n",
    "\n",
    "# # List of websites to scrape (19 websites with similar structure)\n",
    "# websites = [\n",
    "#     {\"main_url\": \"https://wuerzburg-mitmachen.de/projekts\", \"base_url\": \"https://wuerzburg-mitmachen.de\"},\n",
    "#     {\"main_url\": \"https://mitmachen.siegburg.de/projekts\", \"base_url\": \"https://mitmachen.siegburg.de\"}, \n",
    "#     {\"main_url\": \"https://mitmachen.jena.de/projekts\", \"base_url\": \"https://mitmachen.jena.de\"},\n",
    "#     {\"main_url\": \"https://mitmachgemeinde.de/projekts\", \"base_url\": \"https://mitmachgemeinde.de\"},\n",
    "#     {\"main_url\": \"https://bamberg-gestalten.de/projekts\", \"base_url\": \"https://bamberg-gestalten.de\"},\n",
    "#     {\"main_url\": \"https://mitmachen-pforzheim.de/projekts\", \"base_url\": \"https://mitmachen-pforzheim.de\"},\n",
    "#     {\"main_url\": \"https://bochum-mitgestalten.de/projekts\", \"base_url\": \"https://bochum-mitgestalten.de\"},\n",
    "#     {\"main_url\": \"https://unser.muenchen.de/projekts\", \"base_url\": \"https://unser.muenchen.de\"},\n",
    "#     {\"main_url\": \"https://mitreden.ilzerland.bayern/projekts\", \"base_url\": \"https://mitreden.ilzerland.bayern\"},\n",
    "#     {\"main_url\": \"https://stutensee-mitwirken.de/projekts\", \"base_url\": \"https://stutensee-mitwirken.de\"},\n",
    "#     {\"main_url\": \"https://consul.unterschleissheim.de/projekts\", \"base_url\": \"https://consul.unterschleissheim.de\"},\n",
    "#     {\"main_url\": \"https://machmit.kempten.de/projekts\", \"base_url\": \"https://machmit.kempten.de\"},\n",
    "#     {\"main_url\": \"https://consul.detmold-mitgestalten.de/projekts\", \"base_url\": \"https://consul.detmold-mitgestalten.de\"},\n",
    "#     {\"main_url\": \"https://flensburg-mitmachen.de/projekts\", \"base_url\": \"https://flensburg-mitmachen.de\"},  # Fixed URL\n",
    "#     {\"main_url\": \"https://mitmachen.amberg.de/projekts\", \"base_url\": \"https://mitmachen.amberg.de\"},\n",
    "#     {\"main_url\": \"https://mitmachen.smarte-region-linz.de/projekts\", \"base_url\": \"https://mitmachen.smarte-region-linz.de\"},\n",
    "#     {\"main_url\": \"https://mitgestalten.trier.de/projekts\", \"base_url\": \"https://mitgestalten.trier.de\"},\n",
    "#     {\"main_url\": \"https://machmit.augsburg.de/projekts\", \"base_url\": \"https://machmit.augsburg.de\"}\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Initialize empty DataFrames for all projects and proposals \n",
    "# all_projects_df = pd.DataFrame()\n",
    "# all_proposals_df = pd.DataFrame()\n",
    "\n",
    "# # Main loop to scrape all websites in the list\n",
    "# for site in websites:\n",
    "#     main_url = site[\"main_url\"]\n",
    "#     base_url = site[\"base_url\"]\n",
    "\n",
    "#     city = extract_city_name(base_url)\n",
    "\n",
    "#     try:\n",
    "#         # Scrape projects and proposals\n",
    "#         projects_df, proposals_df = scrape_projects_with_proposals(main_url, base_url)\n",
    "\n",
    "#         # Add a 'City' column to all DataFrames\n",
    "#         projects_df['City'] = city\n",
    "#         proposals_df['City'] = city\n",
    "\n",
    "#         # Append results to the combined DataFrames\n",
    "#         all_projects_df = pd.concat([all_projects_df, projects_df], ignore_index=True)\n",
    "#         all_proposals_df = pd.concat([all_proposals_df, proposals_df], ignore_index=True)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to scrape {main_url} - {e}\")\n",
    "\n",
    "\n",
    "# # Save the results to CSV files\n",
    "# all_projects_df.to_csv('all_projects.csv', index=False)\n",
    "# all_proposals_df.to_csv('all_proposals.csv', index=False)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_projects_df = pd.read_csv('all_projects.csv')\n",
    "all_proposals_df = pd.read_csv('all_proposals.csv')\n",
    "all_comments_df = pd.read_csv('all_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City\n",
       "Muenchen             120\n",
       "Siegburg              96\n",
       "Amberg                79\n",
       "Detmold               76\n",
       "Kempten               69\n",
       "Mitren                55\n",
       "Wuerzburg             39\n",
       "Pforzheim             35\n",
       "Bamberg               32\n",
       "Unterschleissheim     27\n",
       "Trier                 20\n",
       "Augsburg              17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_proposals_df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project URL</th>\n",
       "      <th>Project Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Proposal Count</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/grombuehl-zukun...</td>\n",
       "      <td>Energetisches Quartierskonzept f√ºr Gromb√ºhl</td>\n",
       "      <td>Gromb√ºhl 2040 - ein SzenarioDie Stra√üen Gromb√º...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/mobilitaetsplan</td>\n",
       "      <td>Mobilit√§tsplan 2040</td>\n",
       "      <td>Mobilit√§tsplan 2040 f√ºr die Stadt W√ºrzburg: Je...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftsregion</td>\n",
       "      <td>Zukunftsregion W√ºrzburg</td>\n",
       "      <td>Zukunftsregion W√ºrzburg: Jetzt aktiv mitgestal...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftskonzept...</td>\n",
       "      <td>Zukunftskonzepte f√ºr die Innenstadt</td>\n",
       "      <td>Wie soll die W√ºrzburger Innenstadt von morgen ...</td>\n",
       "      <td>24</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/klimaanpassung</td>\n",
       "      <td>Klimaanpassung</td>\n",
       "      <td>Klimaanpassungsstrategie f√ºr die Stadt W√ºrzbur...</td>\n",
       "      <td>14</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Project URL  \\\n",
       "0  https://wuerzburg-mitmachen.de/grombuehl-zukun...   \n",
       "1     https://wuerzburg-mitmachen.de/mobilitaetsplan   \n",
       "2      https://wuerzburg-mitmachen.de/zukunftsregion   \n",
       "3  https://wuerzburg-mitmachen.de/zukunftskonzept...   \n",
       "4      https://wuerzburg-mitmachen.de/klimaanpassung   \n",
       "\n",
       "                                 Project Title  \\\n",
       "0  Energetisches Quartierskonzept f√ºr Gromb√ºhl   \n",
       "1                          Mobilit√§tsplan 2040   \n",
       "2                      Zukunftsregion W√ºrzburg   \n",
       "3          Zukunftskonzepte f√ºr die Innenstadt   \n",
       "4                               Klimaanpassung   \n",
       "\n",
       "                                 Project Description  Proposal Count  \\\n",
       "0  Gromb√ºhl 2040 - ein SzenarioDie Stra√üen Gromb√º...               0   \n",
       "1  Mobilit√§tsplan 2040 f√ºr die Stadt W√ºrzburg: Je...               0   \n",
       "2  Zukunftsregion W√ºrzburg: Jetzt aktiv mitgestal...               0   \n",
       "3  Wie soll die W√ºrzburger Innenstadt von morgen ...              24   \n",
       "4  Klimaanpassungsstrategie f√ºr die Stadt W√ºrzbur...              14   \n",
       "\n",
       "        City  \n",
       "0  Wuerzburg  \n",
       "1  Wuerzburg  \n",
       "2  Wuerzburg  \n",
       "3  Wuerzburg  \n",
       "4  Wuerzburg  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_projects_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BurgerBudgets in Jena (2024, 23, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URLs for the budgets\n",
    "# budget_urls = {\n",
    "#     2024: \"https://mitmachen.jena.de/buergerbudget\",\n",
    "#     2023: \"https://mitmachen.jena.de/buergerbudget-2023\",\n",
    "#     2022: \"https://mitmachen.jena.de/buergerbudget-2022\"\n",
    "# }\n",
    "\n",
    "# # Updated function to scrape and clean a budget table for a given year\n",
    "# def scrape_and_clean_budget_table(url, year):\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to load URL: {url}\")\n",
    "#         return None\n",
    "    \n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     table = soup.find('table', id='budget-investments-compatible')  # Locate the table by its ID\n",
    "    \n",
    "#     if not table:\n",
    "#         print(f\"No table found for URL: {url}\")\n",
    "#         return None\n",
    "    \n",
    "#     # Extract the total available budget for the year (last <th> in <thead>)\n",
    "#     available_budget_tag = table.find('thead').find_all('th')[-1]  # Find the last <th>\n",
    "#     available_budget = (\n",
    "#         float(re.sub(r'[^\\d.]', '', available_budget_tag.get_text(strip=True))) * 1000\n",
    "#         if available_budget_tag else None\n",
    "#     )\n",
    "    \n",
    "#     # Extract table headers\n",
    "#     headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')]\n",
    "    \n",
    "#     # Extract table rows\n",
    "#     rows = []\n",
    "#     for tr in table.find('tbody').find_all('tr'):\n",
    "#         # Extract row cells\n",
    "#         cells = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "        \n",
    "#         # Check the class of the <tr> tag for \"success\" or \"discarded\"\n",
    "#         approved = 1 if 'success' in tr.get('class', []) else 0\n",
    "        \n",
    "#         # Append cells and approval status\n",
    "#         rows.append(cells + [approved])\n",
    "    \n",
    "#     # Add \"Approved\" column to the headers\n",
    "#     headers.append('Approved')\n",
    "    \n",
    "#     # Create a DataFrame\n",
    "#     df = pd.DataFrame(rows, columns=headers)\n",
    "#     df['Year'] = year  # Add a 'Year' column\n",
    "#     df['Available Budget'] = available_budget  # Add the total budget for the year to every row\n",
    "#     return df\n",
    "\n",
    "# # Scrape and clean tables for all years\n",
    "# budget_dataframes = [\n",
    "#     scrape_and_clean_budget_table(url, year) for year, url in budget_urls.items()\n",
    "# ]\n",
    "\n",
    "# # Combine all dataframes into one\n",
    "# budget_jena_df = pd.concat(budget_dataframes, ignore_index=True)\n",
    "\n",
    "# # Clean and transform the DataFrame\n",
    "# budget_jena_df['Preis'] = budget_jena_df['Preis'].str.extract(r'(\\d+)').astype(float) * 1000\n",
    "# budget_jena_df['Stimmen'] = budget_jena_df['Stimmen'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# # Rename columns to English\n",
    "# budget_jena_df.rename(columns={\n",
    "#     'Vorschlag Titel': 'Proposal Title',\n",
    "#     'Stimmen': 'Votes',\n",
    "#     'Preis': 'Price',\n",
    "#     'Year': 'Year',\n",
    "#     'Available Budget': 'Budget for this year',\n",
    "#     'Approved': 'Approved'\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Drop unnecessary columns if any remain\n",
    "# budget_jena_df = budget_jena_df.loc[:, ~budget_jena_df.columns.str.contains('Verf√ºgbareBudgetmittel', na=False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments Scraper\n",
    "\n",
    "This scraper extracts comments for all projects from the `all_projects_df` DataFrame and organizes them into a structured DataFrame:\n",
    "\n",
    "1. **`df_comments`**: Contains all comments associated with projects.\n",
    "   - Columns:\n",
    "     - `URL`: The URL of the project the comment is associated with.\n",
    "     - `Project`: The title of the project the comment is associated with.\n",
    "     - `City`: The city the project belongs to (extracted from the URL).\n",
    "     - `Text`: The content of the comment.\n",
    "     - `Username`: The name of the user who posted the comment.\n",
    "     - `Date`: The date the comment was posted.\n",
    "     - `Likes`: The number of likes the comment received.\n",
    "     - `Dislikes`: The number of dislikes the comment received.\n",
    "     - `Total Votes`: The total votes (likes + dislikes) the comment received.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Good scraper for comments (748 entries comments from 19 cities)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# # Here is the function to extract comments from a single page\n",
    "# def extract_comments_from_page(soup):\n",
    "\n",
    "#     # Initialize an empty list to store the comments and look for the comments section div with class 'comment small-12' (all the comment blocks in similar websites have this class)\n",
    "#     comments_data = []\n",
    "#     comments_section = soup.find_all('div', class_='comment small-12')\n",
    "    \n",
    "#     for comment in comments_section:\n",
    "#         # Extract comment text (clean and remove extra whitespace) by finding the first 'p' tag inside the comment div\n",
    "#         comment_text = comment.find('p').get_text(strip=True) if comment.find('p') else None\n",
    "        \n",
    "#         # Extract username by finding the 'span' tag with class 'user-name' inside the comment div\n",
    "#         username_tag = comment.find('span', class_='user-name')\n",
    "#         username = username_tag.get_text(strip=True) if username_tag else None\n",
    "        \n",
    "#         # Extract date by finding the last 'a' tag inside the 'div' tag with class 'comment-info' (last link is the date)\n",
    "#         date_tag = comment.find('div', class_='comment-info').find_all('a')[-1]\n",
    "#         date = date_tag.get_text(strip=True) if date_tag else None\n",
    "\n",
    "        \n",
    "#         # Extract likes and dislikes (clean and convert to integer) if they exist by finding the 'span' tags with class 'in-favor' and 'against'\n",
    "#         likes_tag = comment.find('span', class_='in-favor')\n",
    "#         likes = int(re.sub(r'\\D', '', likes_tag.get_text(strip=True))) if likes_tag else 0\n",
    "        \n",
    "#         dislikes_tag = comment.find('span', class_='against')\n",
    "#         dislikes = int(re.sub(r'\\D', '', dislikes_tag.get_text(strip=True))) if dislikes_tag else 0\n",
    "        \n",
    "#         # Extract total votes (clean and convert to integer), it was the easiest way to get the total votes\n",
    "#         total_votes = likes + dislikes\n",
    "        \n",
    "#         # Append the extracted information to the comments_data list\n",
    "#         comments_data.append({\n",
    "#             'Text': comment_text,\n",
    "#             'Username': username,\n",
    "#             'Date': date,\n",
    "#             'Likes': likes,\n",
    "#             'Dislikes': dislikes,\n",
    "#             'Total Votes': total_votes\n",
    "#         })\n",
    "#     return comments_data\n",
    "\n",
    "\n",
    "# # Function to extract city name from the base URL to add column 'City' to the comments DataFrame\n",
    "# def extract_city_name(base_url):\n",
    "#     # Words to remove from the city name (most URLs have similar structure and contain these words)\n",
    "#     remove_words = ['mitmachen', 'Mitmachen', 'mitwirken', 'Smarte', 'region', 'unser', 'mitgestalten', 'gestalten', 'machmit', 'dialog', 'consul', 'www', 'de', 'https', 'com']\n",
    "\n",
    "#     # Split the URL into parts (by '.' or '/')\n",
    "#     parts = base_url.replace('https://', '').replace('http://', '').split('.')\n",
    "#     all_parts = [part.split('/')[0] for part in parts]  # Handle cases where \"/\" exists after domain\n",
    "\n",
    "#     # Remove known unwanted words and empty strings\n",
    "#     filtered_parts = [part for part in all_parts if part.lower() not in remove_words and part]\n",
    "\n",
    "#     # Return the first relevant part (assumes city name is left after filtering)\n",
    "#     city = filtered_parts[0].replace('-', ' ').capitalize() if filtered_parts else \"Unknown\"\n",
    "\n",
    "#     # Remove unwanted words from city name\n",
    "#     for word in remove_words:\n",
    "#         city = city.replace(word, '')\n",
    "    \n",
    "\n",
    "#     return city.strip().capitalize()\n",
    "\n",
    "\n",
    "# # Scrape all comments from a paginated URL (e.g., https://example.com/comments?page=1), stop when no comments are found. So basically, this function scrapes all comments from all pages of a project\n",
    "# def scrape_all_comments(base_url):\n",
    "#     comments = []\n",
    "#     page = 1\n",
    "    \n",
    "#     # This while loop will continue until there are no comments on the page (extract_comments_from_page returns an empty list)\n",
    "#     while True:\n",
    "#         paginated_url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "#         response = requests.get(paginated_url)\n",
    "        \n",
    "#         if response.status_code != 200:\n",
    "#             print(f\"Failed to load page {page} for URL: {base_url}\")\n",
    "#             break\n",
    "        \n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         comments_on_page = extract_comments_from_page(soup)\n",
    "        \n",
    "#         if not comments_on_page:  # Stop if no comments on the page (extract_comments_from_page returns an empty list)\n",
    "#             break\n",
    "        \n",
    "#         # Extend the comments list with the comments from the current page and increment the page number\n",
    "#         comments.extend(comments_on_page)\n",
    "#         page += 1\n",
    "\n",
    "#     return comments\n",
    "\n",
    "# # Function to scrape the main content and comments for each URL \n",
    "# # This function is mainly used to scrape the main content and comments for all project URLs and call the scrape_all_comments function to get all comments for each project\n",
    "# # So usage scheme is: \n",
    "# # scrape_content_and_comments(to get main content and comments) -> scrape_all_comments(to get all comments for each project) \n",
    "# # -> extract_comments_from_page(to get comments from a single page) \n",
    "# # -> extract_city_name(to get city name from URL) \n",
    "# # -> form the final DataFrame with comments\n",
    "\n",
    "# def scrape_content_and_comments(urls):\n",
    "#     data = []\n",
    "    \n",
    "#     for url in urls:\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code != 200:\n",
    "#             print(f\"Failed to load URL: {url}\")\n",
    "#             continue\n",
    "        \n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "#         # Scrape main content (title and description) by finding the 'title' tag and the 'div' tag with class 'flex-layout'\n",
    "#         title = soup.find('title').get_text(strip=True) if soup.find('title') else None\n",
    "#         content_div = soup.find('div', class_='flex-layout')\n",
    "#         content = content_div.get_text(strip=True) if content_div else None\n",
    "        \n",
    "#         # Scrape comments by calling the scrape_all_comments function\n",
    "#         comments = scrape_all_comments(url)\n",
    "        \n",
    "#         # Append the extracted information to the data list\n",
    "#         data.append({\n",
    "#             'URL': url,\n",
    "#             'Title': title,\n",
    "#             'Content': content,\n",
    "#             'Comments': comments\n",
    "#         })\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# # Scrape comments for all project URLs from all_projects_df (created in the previous step)\n",
    "# urls = all_projects_df['Project URL'].tolist()  # Use the 'Project URL' column from all_projects_df\n",
    "# scraped_data = scrape_content_and_comments(urls)\n",
    "\n",
    "# # Create structured DataFrame for comments\n",
    "# comments_data = []\n",
    "\n",
    "# # Loop through the scraped data and extract comments, link them to the project URL and extract the city name\n",
    "# for item in scraped_data:\n",
    "#     for comment in item['Comments']:\n",
    "#         comment['URL'] = item['URL']  # Link comment to the project URL\n",
    "#         # Extract city name from URL\n",
    "#         city = extract_city_name(item['URL'])\n",
    "#         comment['City'] = city\n",
    "#         comments_data.append(comment)\n",
    "\n",
    "# # Create the comments DataFrame\n",
    "# df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# # Create a mapping from URL to Project Title (mapping means that we can use the URL to get the Project Title)\n",
    "# url_to_title = all_projects_df.set_index('Project URL')['Project Title'].to_dict()\n",
    "\n",
    "# # Add a 'Project' column to df_comments using the mapping (again method map is working like that: it takes the URL and returns the Project Title)\n",
    "# df_comments['Project'] = df_comments['URL'].map(url_to_title)\n",
    "# df_comments = df_comments[['URL', 'Project', 'City'] + [col for col in df_comments.columns if col not in ['URL', 'Project', 'City']]]\n",
    "\n",
    "# # Save the comments DataFrame to a CSV file\n",
    "\n",
    "# df_comments.to_csv('all_comments.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments_df['Total Votes'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cleaaning and structuring for Sieburg (review if it's needed) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Enhanced function to extract all logical parts, including \"Unterst√ºtzer*innen\"\n",
    "# def extract_full_data_with_supporters(content):\n",
    "#     # Extract title (everything before the first date)\n",
    "#     title_match = re.search(r'^(.*?)(\\r|\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "#     title = title_match.group(1).strip() if title_match else None\n",
    "\n",
    "#     # Extract date\n",
    "#     date_match = re.search(r'\\d{1,2}\\.\\s\\w+\\s\\d{4}', content)\n",
    "#     date = date_match.group(0) if date_match else None\n",
    "\n",
    "#     # Extract comments count\n",
    "#     comments_match = re.search(r'(\\d+)\\sKommentare', content)\n",
    "#     comments = int(comments_match.group(1)) if comments_match else 0\n",
    "\n",
    "#     # Extract tags (sections with numbers or + signs)\n",
    "#     tags_match = re.findall(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "#     tags = ', '.join(tags_match) if tags_match else None\n",
    "\n",
    "#     # Extract description (everything after \"Geselliges Beisammensein\" or similar patterns)\n",
    "#     description_start = re.search(r'(Geselliges Beisammensein|Angebotslandkarte)', content)\n",
    "#     description = content[description_start.start():].strip() if description_start else None\n",
    "\n",
    "#     # Extract username\n",
    "#     username_match = re.search(r'(\\w+\\s\\w+|Beigetreten am:.*?\\d{4})', content)\n",
    "#     username = username_match.group(1).split('Beigetreten am:')[0].strip() if username_match else None\n",
    "\n",
    "#     # Extract Vorschl√§ge count\n",
    "#     vorschlaege_match = re.search(r'Vorschl√§ge(\\d+)', content)\n",
    "#     vorschlaege = int(vorschlaege_match.group(1)) if vorschlaege_match else 0\n",
    "\n",
    "#     # Extract Konto verification status\n",
    "#     konto_match = re.search(r'(Konto\\s(verifiziert|ist nicht verifiziert))', content)\n",
    "#     konto_status = konto_match.group(2) if konto_match else None\n",
    "\n",
    "#     # # Extract registration date\n",
    "#     # registration_match = re.search(r'Beigetreten am:\\s(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "#     # registration_date = registration_match.group(1) if registration_match else None\n",
    "\n",
    "#     # Extract number of Unterst√ºtzer*innen\n",
    "#     supporters_match = re.search(r'(\\d+)\\sUnterst√ºtzer\\*in', content)\n",
    "#     supporters = int(supporters_match.group(1)) if supporters_match else 0\n",
    "\n",
    "#     return title, date, comments, tags, description, username, vorschlaege, konto_status, supporters\n",
    "\n",
    "# # Apply the enhanced function to the DataFrame and create new columns\n",
    "# df_sieburg[['Title', 'Date', 'Comments', 'Tags', 'Description', 'Username', 'Vorschl√§ge', 'Konto Status', 'Supporters']] = df_sieburg['Content'].apply(\n",
    "#     lambda x: pd.Series(extract_full_data_with_supporters(x))\n",
    "# )\n",
    "\n",
    "\n",
    "# # Function to clean description considering keywords, numeric patterns, and refined starting logic\n",
    "# def clean_description_advanced(content):\n",
    "#     # Define keywords that mark the beginning of the description\n",
    "#     keywords = [\n",
    "#         'Geselliges Beisammensein', 'Natur', 'Hilfe & Beratung', 'Bildung', \n",
    "#         'Musik', 'Bewegung', 'Glaube', 'Kulinarisches', 'Kunst & Kultur', 'Sonstiges',\n",
    "#     ]\n",
    "    \n",
    "#     # Check for keywords first\n",
    "#     for keyword in keywords:\n",
    "#         if keyword in content:\n",
    "#             start_idx = content.find(keyword) + len(keyword)\n",
    "#             description = content[start_idx:].strip()\n",
    "#             description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#             return description\n",
    "\n",
    "#     # If no keyword is found, check for numeric patterns like \"18-24, 25-49, etc.\"\n",
    "#     numeric_pattern = re.search(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "#     if numeric_pattern:\n",
    "#         start_idx = numeric_pattern.end()\n",
    "#         description = content[start_idx:].strip()\n",
    "#         description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#         return description\n",
    "\n",
    "#     # As a fallback, find the first capital letter, quote, or digit to mark the start\n",
    "#     fallback_match = re.search(r'[A-Z\"0-9]', content)\n",
    "#     if fallback_match:\n",
    "#         start_idx = fallback_match.start()\n",
    "#         description = content[start_idx:].strip()\n",
    "#         description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#         return description\n",
    "\n",
    "#     # If nothing works, return the content as is\n",
    "#     return content\n",
    "\n",
    "# # Apply the advanced cleaning function to the Description column\n",
    "# df_sieburg['Description'] = df_sieburg['Content'].apply(clean_description_advanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AI Report Generated: jena_ai_report.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from fpdf import FPDF\n",
    "\n",
    "# üîë Set your Hugging Face API key\n",
    "HF_API_KEY = \"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\"  # Replace with actual API key\n",
    "HF_MODEL = \"tiiuae/falcon-7b-instruct\"  # Alternative: \"mistralai/Mistral-7B-Instruct\"\n",
    "\n",
    "# Load German NLP model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Load CSV data\n",
    "comments_df = pd.read_csv(\"all_comments.csv\")\n",
    "projects_df = pd.read_csv(\"all_projects.csv\")\n",
    "proposals_df = pd.read_csv(\"all_proposals.csv\")\n",
    "\n",
    "# üîπ Filter Data for Jena\n",
    "jena_comments = comments_df[comments_df[\"City\"] == \"Jena\"].copy()\n",
    "jena_projects = projects_df[projects_df[\"City\"] == \"Jena\"].copy()\n",
    "jena_proposals = proposals_df[proposals_df[\"City\"] == \"Jena\"].copy()\n",
    "\n",
    "# üîπ 1. Most Active Users\n",
    "most_active_users = jena_comments[\"Username\"].value_counts().head(10)\n",
    "\n",
    "# üîπ 2. Most Commented Projects\n",
    "most_commented_projects = jena_comments[\"Project\"].value_counts().head(5)\n",
    "\n",
    "# üîπ 3. Sentiment Analysis\n",
    "jena_comments.loc[:, \"Sentiment\"] = jena_comments[\"Text\"].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "jena_comments.loc[:, \"Sentiment Category\"] = jena_comments[\"Sentiment\"].apply(lambda x: \"Positive\" if x > 0 else (\"Negative\" if x < 0 else \"Neutral\"))\n",
    "sentiment_counts = jena_comments[\"Sentiment Category\"].value_counts()\n",
    "\n",
    "# üîπ 4. NLP Analysis with spaCy (Topic Extraction)\n",
    "def extract_keywords(text):\n",
    "    doc = nlp(str(text))\n",
    "    keywords = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return keywords\n",
    "\n",
    "jena_comments.loc[:, \"Keywords\"] = jena_comments[\"Text\"].apply(extract_keywords)\n",
    "all_keywords = [keyword for keywords in jena_comments[\"Keywords\"] for keyword in keywords]\n",
    "keyword_counts = Counter(all_keywords).most_common(20)\n",
    "\n",
    "# üîπ 5. Generate Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(all_keywords))\n",
    "wordcloud_path = \"jena_wordcloud.png\"\n",
    "wordcloud.to_file(wordcloud_path)\n",
    "\n",
    "# üîπ 6. Generate Activity Charts\n",
    "plt.figure(figsize=(8, 5))\n",
    "most_active_users.plot(kind=\"bar\", color=\"blue\")\n",
    "plt.title(\"Top 10 Most Active Users in Jena\")\n",
    "plt.xlabel(\"Username\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "active_users_plot_path = \"jena_active_users.png\"\n",
    "plt.savefig(active_users_plot_path)\n",
    "plt.close()\n",
    "\n",
    "# Sentiment distribution chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "sentiment_counts.plot(kind=\"bar\", color=[\"green\", \"gray\", \"red\"])\n",
    "plt.title(\"Sentiment Distribution of Comments in Jena\")\n",
    "plt.xlabel(\"Sentiment Category\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "sentiment_plot_path = \"jena_sentiment_distribution.png\"\n",
    "plt.savefig(sentiment_plot_path)\n",
    "plt.close()\n",
    "\n",
    "# üîπ 7. Generate AI-Powered Summary via Hugging Face API\n",
    "jena_data_summary = f\"\"\"\n",
    "City: Jena\n",
    "Total Projects: {len(jena_projects)}\n",
    "Total Proposals: {len(jena_proposals)}\n",
    "Total Comments: {len(jena_comments)}\n",
    "\n",
    "Top 5 Most Commented Projects:\n",
    "{most_commented_projects.to_string()}\n",
    "\n",
    "Top 10 Most Active Users:\n",
    "{most_active_users.to_string()}\n",
    "\n",
    "Sentiment Analysis:\n",
    "- Neutral Comments: {sentiment_counts.get('Neutral', 0)}\n",
    "- Positive Comments: {sentiment_counts.get('Positive', 0)}\n",
    "- Negative Comments: {sentiment_counts.get('Negative', 0)}\n",
    "\n",
    "Most Common Discussion Topics:\n",
    "{', '.join([word for word, count in keyword_counts])}\n",
    "\"\"\"\n",
    "\n",
    "ai_prompt = f\"\"\"\n",
    "Generate a structured analytical report on civic engagement in Jena based on the following insights:\n",
    "\n",
    "{jena_data_summary}\n",
    "\n",
    "The report should include:\n",
    "- A professional introduction about civic engagement in Jena.\n",
    "- Key trends and insights from the provided data.\n",
    "- Observations on public sentiment and discussion topics.\n",
    "- Suggestions for improving citizen engagement.\n",
    "\n",
    "Ensure the report is structured, formal, and insightful.\n",
    "\"\"\"\n",
    "\n",
    "def generate_report(text):\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
    "    payload = {\"inputs\": text, \"parameters\": {\"max_new_tokens\": 1000}}\n",
    "\n",
    "    response = requests.post(f\"https://api-inference.huggingface.co/models/{HF_MODEL}\", json=payload, headers=headers)\n",
    "    \n",
    "    try:\n",
    "        result = response.json()\n",
    "        if \"error\" in result:\n",
    "            return f\"‚ùå Error from Hugging Face API: {result['error']}\"\n",
    "        elif isinstance(result, list) and \"generated_text\" in result[0]:\n",
    "            return result[0][\"generated_text\"]\n",
    "        else:\n",
    "            return \"‚ùå Unexpected API response format.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Exception occurred: {str(e)}\"\n",
    "\n",
    "# Call AI for Summary\n",
    "ai_generated_report = generate_report(ai_prompt)\n",
    "\n",
    "# Create PDF Report\n",
    "pdf = FPDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "pdf.add_page()\n",
    "\n",
    "# ‚úÖ Add Unicode Font\n",
    "pdf.add_font(\"Arial\", \"\", \"C:\\\\Windows\\\\Fonts\\\\arial.ttf\", uni=True)  # Windows path\n",
    "pdf.set_font(\"Arial\", \"\", 12)\n",
    "\n",
    "pdf.set_font(\"Arial\", \"B\", 16)\n",
    "pdf.cell(200, 10, \"AI-Generated Analytical Report for Jena\", ln=True, align=\"C\")\n",
    "\n",
    "pdf.set_font(\"Arial\", \"B\", 12)\n",
    "pdf.cell(200, 10, \"Summary of Engagement Data\", ln=True)\n",
    "\n",
    "pdf.set_font(\"Arial\", \"\", 12)\n",
    "pdf.multi_cell(190, 10, jena_data_summary)  # Insert structured insights\n",
    "\n",
    "pdf.set_font(\"Arial\", \"B\", 12)\n",
    "pdf.cell(200, 10, \"AI-Generated Analysis\", ln=True)\n",
    "\n",
    "pdf.set_font(\"Arial\", \"\", 12)\n",
    "pdf.multi_cell(190, 10, ai_generated_report)  # Insert AI-generated content\n",
    "\n",
    "\n",
    "# Insert Charts\n",
    "pdf.image(wordcloud_path, x=50, w=100)\n",
    "pdf.image(active_users_plot_path, x=50, w=100)\n",
    "pdf.image(sentiment_plot_path, x=50, w=100)\n",
    "\n",
    "# Save PDF (overwrite previous one)\n",
    "pdf_path = \"jena_ai_report.pdf\"\n",
    "pdf.output(pdf_path, \"F\")\n",
    "\n",
    "print(f\"‚úÖ AI Report Generated: {pdf_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
