{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper for 19 Cities\n",
    "\n",
    "This scraper extracts and organizes data into three main DataFrames:\n",
    "1. **`all_projects_df`**: Contains all projects from the websites.\n",
    "   - Columns: `Project URL`, `Project Title`, `Project Description`, `Proposal Count`, `City`\n",
    "\n",
    "2. **`all_proposals_df`**: Contains all proposals under projects.\n",
    "   - Columns: `URL`, `Title`, `Proposed for Project`, `Description`, `Author`, `Comments`, `Supporters`, `City`\n",
    "\n",
    "3. **`all_comments_df`**: Contains all comments under projects and proposals.\n",
    "   - Columns: `URL`, `Project`, `Text`, `Author`, `Likes`, `Dislikes`, `Date`, `City`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping project at https://flensburg-mitmachen.dehttps://survey.lamapoll.de/Publikumspreis-Kommune-bewegt-Welt-2024: HTTPSConnectionPool(host='flensburg-mitmachen.dehttps', port=443): Max retries exceeded with url: /survey.lamapoll.de/Publikumspreis-Kommune-bewegt-Welt-2024 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000018D29CE5AB0>: Failed to resolve 'flensburg-mitmachen.dehttps' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Updated function to extract proposals from a project page\n",
    "def extract_proposals(soup, base_url):\n",
    "    proposals = []\n",
    "    proposal_items = soup.find_all('div', class_='resource-item proposal-list-item')\n",
    "\n",
    "    for proposal in proposal_items:\n",
    "        # Extract title\n",
    "        title_tag = proposal.find('a', class_='resource-item--title')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "        # Extract URL\n",
    "        url = base_url + title_tag['href'] if title_tag and 'href' in title_tag.attrs else None\n",
    "\n",
    "        # Extract description\n",
    "        description_tag = proposal.find('div', class_='resource-item--description')\n",
    "        description = description_tag.get_text(strip=True) if description_tag else None\n",
    "\n",
    "        # Extract author/username\n",
    "        author_tag = proposal.find('a', class_='resource-item--author')\n",
    "        author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "        # Extract number of comments\n",
    "        comments_tag = proposal.find('span', class_='comments')\n",
    "        comments = int(comments_tag.get_text(strip=True).split()[0]) if comments_tag else 0\n",
    "\n",
    "        # Extract number of supporters\n",
    "        supporters_tag = proposal.find('span', class_='total-supports')\n",
    "        supporters = int(supporters_tag.get_text(strip=True).split()[0]) if supporters_tag else 0\n",
    "\n",
    "        # Extract parent project\n",
    "        project_tag = proposal.find('a', class_='breadcrumbs-item')\n",
    "        proposed_for_project = project_tag.get_text(strip=True) if project_tag else None\n",
    "\n",
    "        proposals.append({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Proposed for Project': proposed_for_project,\n",
    "            'Description': description,\n",
    "            'Author': author,\n",
    "            'Comments': comments,\n",
    "            'Supporters': supporters,\n",
    "        })\n",
    "    return proposals\n",
    "\n",
    "\n",
    "# Function to extract city name from the base URL\n",
    "def extract_city_name(base_url):\n",
    "    # Words to remove from the city name\n",
    "    remove_words = ['mitmachen', 'Mitmachen', 'mitwirken', 'Smarte', 'region', 'unser', 'mitgestalten', 'gestalten', 'machmit', 'dialog', 'consul', 'www', 'de', 'https', 'com']\n",
    "\n",
    "    # Split the URL into parts (by '.' or '/')\n",
    "    parts = base_url.replace('https://', '').replace('http://', '').split('.')\n",
    "    all_parts = [part.split('/')[0] for part in parts]  # Handle cases where \"/\" exists after domain\n",
    "\n",
    "    # Remove known unwanted words and empty strings\n",
    "    filtered_parts = [part for part in all_parts if part.lower() not in remove_words and part]\n",
    "\n",
    "    # Return the first relevant part (assumes city name is left after filtering)\n",
    "    city = filtered_parts[0].replace('-', ' ').capitalize() if filtered_parts else \"Unknown\"\n",
    "\n",
    "    # Remove unwanted words from city name\n",
    "    for word in remove_words:\n",
    "        city = city.replace(word, '')\n",
    "\n",
    "    return city.strip().capitalize()\n",
    "\n",
    "\n",
    "# Update project scraping to exclude comments\n",
    "def scrape_project_page_with_proposals(url, base_url, city):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load project page: {url}\")\n",
    "        return None, []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract project title\n",
    "    title_tag = soup.find('title')\n",
    "    project_title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "    # Extract project description\n",
    "    content_div = soup.find('div', class_='flex-layout')\n",
    "    description = content_div.get_text(strip=True) if content_div else None\n",
    "\n",
    "    # Extract proposals\n",
    "    proposals = extract_proposals(soup, base_url=base_url)\n",
    "\n",
    "    return {\n",
    "        'Project URL': url,\n",
    "        'Project Title': project_title,\n",
    "        'Project Description': description,\n",
    "        'Proposal Count': len(proposals),\n",
    "    }, proposals\n",
    "\n",
    "\n",
    "# Modified function to scrape projects with proposals\n",
    "def scrape_projects_with_proposals(main_url, base_url):\n",
    "    response = requests.get(main_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load main projects page: {main_url}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all project links\n",
    "    links = soup.find_all('a', class_='resource-item--title')\n",
    "    project_links = [base_url + link['href'] for link in links if 'href' in link.attrs]\n",
    "\n",
    "    projects = []\n",
    "    all_proposals = []\n",
    "\n",
    "    for project_url in project_links:\n",
    "        try:\n",
    "            project_data, proposals = scrape_project_page_with_proposals(\n",
    "                project_url, base_url, extract_city_name(base_url)\n",
    "            )\n",
    "            if project_data:\n",
    "                projects.append(project_data)\n",
    "                all_proposals.extend(proposals)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping project at {project_url}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(projects), pd.DataFrame(all_proposals)\n",
    "\n",
    "# List of websites (fixed flensburg-mitmachen.de base_url)\n",
    "websites = [\n",
    "    {\"main_url\": \"https://wuerzburg-mitmachen.de/projekts\", \"base_url\": \"https://wuerzburg-mitmachen.de\"},\n",
    "    {\"main_url\": \"https://mitmachen.siegburg.de/projekts\", \"base_url\": \"https://mitmachen.siegburg.de\"}, \n",
    "    {\"main_url\": \"https://mitmachen.jena.de/projekts\", \"base_url\": \"https://mitmachen.jena.de\"},\n",
    "    {\"main_url\": \"https://mitmachgemeinde.de/projekts\", \"base_url\": \"https://mitmachgemeinde.de\"},\n",
    "    {\"main_url\": \"https://bamberg-gestalten.de/projekts\", \"base_url\": \"https://bamberg-gestalten.de\"},\n",
    "    {\"main_url\": \"https://mitmachen-pforzheim.de/projekts\", \"base_url\": \"https://mitmachen-pforzheim.de\"},\n",
    "    {\"main_url\": \"https://bochum-mitgestalten.de/projekts\", \"base_url\": \"https://bochum-mitgestalten.de\"},\n",
    "    {\"main_url\": \"https://unser.muenchen.de/projekts\", \"base_url\": \"https://unser.muenchen.de\"},\n",
    "    {\"main_url\": \"https://mitreden.ilzerland.bayern/projekts\", \"base_url\": \"https://mitreden.ilzerland.bayern\"},\n",
    "    {\"main_url\": \"https://stutensee-mitwirken.de/projekts\", \"base_url\": \"https://stutensee-mitwirken.de\"},\n",
    "    {\"main_url\": \"https://consul.unterschleissheim.de/projekts\", \"base_url\": \"https://consul.unterschleissheim.de\"},\n",
    "    {\"main_url\": \"https://machmit.kempten.de/projekts\", \"base_url\": \"https://machmit.kempten.de\"},\n",
    "    {\"main_url\": \"https://consul.detmold-mitgestalten.de/projekts\", \"base_url\": \"https://consul.detmold-mitgestalten.de\"},\n",
    "    {\"main_url\": \"https://flensburg-mitmachen.de/projekts\", \"base_url\": \"https://flensburg-mitmachen.de\"},  # Fixed URL\n",
    "    {\"main_url\": \"https://mitmachen.amberg.de/projekts\", \"base_url\": \"https://mitmachen.amberg.de\"},\n",
    "    {\"main_url\": \"https://mitmachen.smarte-region-linz.de/projekts\", \"base_url\": \"https://mitmachen.smarte-region-linz.de\"},\n",
    "    {\"main_url\": \"https://mitgestalten.trier.de/projekts\", \"base_url\": \"https://mitgestalten.trier.de\"},\n",
    "    {\"main_url\": \"https://machmit.augsburg.de/projekts\", \"base_url\": \"https://machmit.augsburg.de\"}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize empty DataFrames for all projects and proposals\n",
    "all_projects_df = pd.DataFrame()\n",
    "all_proposals_df = pd.DataFrame()\n",
    "\n",
    "# Main loop to scrape all websites\n",
    "for site in websites:\n",
    "    main_url = site[\"main_url\"]\n",
    "    base_url = site[\"base_url\"]\n",
    "\n",
    "    city = extract_city_name(base_url)\n",
    "\n",
    "    try:\n",
    "        # Scrape projects and proposals\n",
    "        projects_df, proposals_df = scrape_projects_with_proposals(main_url, base_url)\n",
    "\n",
    "        # Add a 'City' column to all DataFrames\n",
    "        projects_df['City'] = city\n",
    "        proposals_df['City'] = city\n",
    "\n",
    "        # Append results to the combined DataFrames\n",
    "        all_projects_df = pd.concat([all_projects_df, projects_df], ignore_index=True)\n",
    "        all_proposals_df = pd.concat([all_proposals_df, proposals_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {main_url} - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Proposed for Project</th>\n",
       "      <th>Description</th>\n",
       "      <th>Author</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Supporters</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/110-autofreier-bischofshut</td>\n",
       "      <td>Autofreier Bischofshut</td>\n",
       "      <td>Zukunftskonzepte f√ºr die Innenstadt</td>\n",
       "      <td>Wir fordern die Ausrufung des Klimanotstands, damit Belange unseres Klimas vor das wirtschaftlic...</td>\n",
       "      <td>Letzte Generation W√ºrzburg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/109-e-scooter-verbieten</td>\n",
       "      <td>E Scooter verbieten</td>\n",
       "      <td>Zukunftskonzepte f√ºr die Innenstadt</td>\n",
       "      <td>E Scooter sollten (im Innenstadtbereich) verboten werden. Diese werden h√§ufig willk√ºrlich abgest...</td>\n",
       "      <td>Ccmuet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/108-barrierefrei-ins-nautiland-lgs</td>\n",
       "      <td>Barrierefrei ins Nautiland/LGS</td>\n",
       "      <td>Zukunftskonzepte f√ºr die Innenstadt</td>\n",
       "      <td>Nautiland - neu.\\r\\nUmweltstation - neu.\\r\\nZellertorauffahrt - neu.\\r\\nLeider fehlen die barrie...</td>\n",
       "      <td>AASeuffert</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/107-kinderabenteuer-indoor-spielplatz-smaland</td>\n",
       "      <td>Kinderabenteuer / Indoor Spielplatz / Smaland</td>\n",
       "      <td>Zukunftskonzepte f√ºr die Innenstadt</td>\n",
       "      <td>Es gibt zwar schon den FunPark f√ºr Kinder mit Trampolinhalle etc. in der N√§he der N√ºrnberger Str...</td>\n",
       "      <td>ABlitz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/proposals/106-banke-und-grun-im-neu-gestalteten-bereich-karmelite...</td>\n",
       "      <td>B√§nke und \"Gr√ºn\" im neu gestalteten Bereich Karmelitenstra√üe/Vierr√∂hr...</td>\n",
       "      <td>Zukunftskonzepte f√ºr die Innenstadt</td>\n",
       "      <td>Die Baustelle von der Karmelitenstra√üe zum Vierr√∂hrenbrunnen wurde vor kurzem abgeschlossen. Die...</td>\n",
       "      <td>Ccmuet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   URL  \\\n",
       "0                                  https://wuerzburg-mitmachen.de/proposals/110-autofreier-bischofshut   \n",
       "1                                     https://wuerzburg-mitmachen.de/proposals/109-e-scooter-verbieten   \n",
       "2                          https://wuerzburg-mitmachen.de/proposals/108-barrierefrei-ins-nautiland-lgs   \n",
       "3               https://wuerzburg-mitmachen.de/proposals/107-kinderabenteuer-indoor-spielplatz-smaland   \n",
       "4  https://wuerzburg-mitmachen.de/proposals/106-banke-und-grun-im-neu-gestalteten-bereich-karmelite...   \n",
       "\n",
       "                                                                      Title  \\\n",
       "0                                                    Autofreier Bischofshut   \n",
       "1                                                       E Scooter verbieten   \n",
       "2                                            Barrierefrei ins Nautiland/LGS   \n",
       "3                             Kinderabenteuer / Indoor Spielplatz / Smaland   \n",
       "4  B√§nke und \"Gr√ºn\" im neu gestalteten Bereich Karmelitenstra√üe/Vierr√∂hr...   \n",
       "\n",
       "                  Proposed for Project  \\\n",
       "0  Zukunftskonzepte f√ºr die Innenstadt   \n",
       "1  Zukunftskonzepte f√ºr die Innenstadt   \n",
       "2  Zukunftskonzepte f√ºr die Innenstadt   \n",
       "3  Zukunftskonzepte f√ºr die Innenstadt   \n",
       "4  Zukunftskonzepte f√ºr die Innenstadt   \n",
       "\n",
       "                                                                                           Description  \\\n",
       "0  Wir fordern die Ausrufung des Klimanotstands, damit Belange unseres Klimas vor das wirtschaftlic...   \n",
       "1  E Scooter sollten (im Innenstadtbereich) verboten werden. Diese werden h√§ufig willk√ºrlich abgest...   \n",
       "2  Nautiland - neu.\\r\\nUmweltstation - neu.\\r\\nZellertorauffahrt - neu.\\r\\nLeider fehlen die barrie...   \n",
       "3  Es gibt zwar schon den FunPark f√ºr Kinder mit Trampolinhalle etc. in der N√§he der N√ºrnberger Str...   \n",
       "4  Die Baustelle von der Karmelitenstra√üe zum Vierr√∂hrenbrunnen wurde vor kurzem abgeschlossen. Die...   \n",
       "\n",
       "                       Author  Comments  Supporters       City  \n",
       "0  Letzte Generation W√ºrzburg       0.0        20.0  Wuerzburg  \n",
       "1                      Ccmuet       0.0         2.0  Wuerzburg  \n",
       "2                  AASeuffert       0.0         2.0  Wuerzburg  \n",
       "3                      ABlitz       0.0         0.0  Wuerzburg  \n",
       "4                      Ccmuet       0.0        12.0  Wuerzburg  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_proposals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project URL</th>\n",
       "      <th>Project Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Proposal Count</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/grombuehl-zukunftssicher</td>\n",
       "      <td>Energetisches Quartierskonzept f√ºr Gromb√ºhl</td>\n",
       "      <td>Gromb√ºhl 2040 - ein SzenarioDie Stra√üen Gromb√ºhls sind gr√ºner, ruhiger und voller Leben. Das ‚ÄûQu...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/mobilitaetsplan</td>\n",
       "      <td>Mobilit√§tsplan 2040</td>\n",
       "      <td>Mobilit√§tsplan 2040 f√ºr die Stadt W√ºrzburg: Jetzt mitmachen!Aktuell erstellt die Stadt W√ºrzburg ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftsregion</td>\n",
       "      <td>Zukunftsregion W√ºrzburg</td>\n",
       "      <td>Zukunftsregion W√ºrzburg: Jetzt aktiv mitgestalten!Die Stadt und der Landkreis W√ºrzburg wollen ih...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftskonzepte-fuer-die-innenstadt</td>\n",
       "      <td>Zukunftskonzepte f√ºr die Innenstadt</td>\n",
       "      <td>Wie soll die W√ºrzburger Innenstadt von morgen aussehen? Was w√ºnschen sich B√ºrger:innen, Einzelh√§...</td>\n",
       "      <td>24</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/klimaanpassung</td>\n",
       "      <td>Klimaanpassung</td>\n",
       "      <td>Klimaanpassungsstrategie f√ºr die Stadt W√ºrzburg: Jetzt mitmachen!W√ºrzburg - Seit Anfang 2024 era...</td>\n",
       "      <td>14</td>\n",
       "      <td>Wuerzburg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Project URL  \\\n",
       "0              https://wuerzburg-mitmachen.de/grombuehl-zukunftssicher   \n",
       "1                       https://wuerzburg-mitmachen.de/mobilitaetsplan   \n",
       "2                        https://wuerzburg-mitmachen.de/zukunftsregion   \n",
       "3  https://wuerzburg-mitmachen.de/zukunftskonzepte-fuer-die-innenstadt   \n",
       "4                        https://wuerzburg-mitmachen.de/klimaanpassung   \n",
       "\n",
       "                                 Project Title  \\\n",
       "0  Energetisches Quartierskonzept f√ºr Gromb√ºhl   \n",
       "1                          Mobilit√§tsplan 2040   \n",
       "2                      Zukunftsregion W√ºrzburg   \n",
       "3          Zukunftskonzepte f√ºr die Innenstadt   \n",
       "4                               Klimaanpassung   \n",
       "\n",
       "                                                                                   Project Description  \\\n",
       "0  Gromb√ºhl 2040 - ein SzenarioDie Stra√üen Gromb√ºhls sind gr√ºner, ruhiger und voller Leben. Das ‚ÄûQu...   \n",
       "1  Mobilit√§tsplan 2040 f√ºr die Stadt W√ºrzburg: Jetzt mitmachen!Aktuell erstellt die Stadt W√ºrzburg ...   \n",
       "2  Zukunftsregion W√ºrzburg: Jetzt aktiv mitgestalten!Die Stadt und der Landkreis W√ºrzburg wollen ih...   \n",
       "3  Wie soll die W√ºrzburger Innenstadt von morgen aussehen? Was w√ºnschen sich B√ºrger:innen, Einzelh√§...   \n",
       "4  Klimaanpassungsstrategie f√ºr die Stadt W√ºrzburg: Jetzt mitmachen!W√ºrzburg - Seit Anfang 2024 era...   \n",
       "\n",
       "   Proposal Count       City  \n",
       "0               0  Wuerzburg  \n",
       "1               0  Wuerzburg  \n",
       "2               0  Wuerzburg  \n",
       "3              24  Wuerzburg  \n",
       "4              14  Wuerzburg  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_projects_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BurgerBudgets in Jena (2024, 23, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URLs for the budgets\n",
    "# budget_urls = {\n",
    "#     2024: \"https://mitmachen.jena.de/buergerbudget\",\n",
    "#     2023: \"https://mitmachen.jena.de/buergerbudget-2023\",\n",
    "#     2022: \"https://mitmachen.jena.de/buergerbudget-2022\"\n",
    "# }\n",
    "\n",
    "# # Updated function to scrape and clean a budget table for a given year\n",
    "# def scrape_and_clean_budget_table(url, year):\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to load URL: {url}\")\n",
    "#         return None\n",
    "    \n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     table = soup.find('table', id='budget-investments-compatible')  # Locate the table by its ID\n",
    "    \n",
    "#     if not table:\n",
    "#         print(f\"No table found for URL: {url}\")\n",
    "#         return None\n",
    "    \n",
    "#     # Extract the total available budget for the year (last <th> in <thead>)\n",
    "#     available_budget_tag = table.find('thead').find_all('th')[-1]  # Find the last <th>\n",
    "#     available_budget = (\n",
    "#         float(re.sub(r'[^\\d.]', '', available_budget_tag.get_text(strip=True))) * 1000\n",
    "#         if available_budget_tag else None\n",
    "#     )\n",
    "    \n",
    "#     # Extract table headers\n",
    "#     headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')]\n",
    "    \n",
    "#     # Extract table rows\n",
    "#     rows = []\n",
    "#     for tr in table.find('tbody').find_all('tr'):\n",
    "#         # Extract row cells\n",
    "#         cells = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "        \n",
    "#         # Check the class of the <tr> tag for \"success\" or \"discarded\"\n",
    "#         approved = 1 if 'success' in tr.get('class', []) else 0\n",
    "        \n",
    "#         # Append cells and approval status\n",
    "#         rows.append(cells + [approved])\n",
    "    \n",
    "#     # Add \"Approved\" column to the headers\n",
    "#     headers.append('Approved')\n",
    "    \n",
    "#     # Create a DataFrame\n",
    "#     df = pd.DataFrame(rows, columns=headers)\n",
    "#     df['Year'] = year  # Add a 'Year' column\n",
    "#     df['Available Budget'] = available_budget  # Add the total budget for the year to every row\n",
    "#     return df\n",
    "\n",
    "# # Scrape and clean tables for all years\n",
    "# budget_dataframes = [\n",
    "#     scrape_and_clean_budget_table(url, year) for year, url in budget_urls.items()\n",
    "# ]\n",
    "\n",
    "# # Combine all dataframes into one\n",
    "# budget_jena_df = pd.concat(budget_dataframes, ignore_index=True)\n",
    "\n",
    "# # Clean and transform the DataFrame\n",
    "# budget_jena_df['Preis'] = budget_jena_df['Preis'].str.extract(r'(\\d+)').astype(float) * 1000\n",
    "# budget_jena_df['Stimmen'] = budget_jena_df['Stimmen'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# # Rename columns to English\n",
    "# budget_jena_df.rename(columns={\n",
    "#     'Vorschlag Titel': 'Proposal Title',\n",
    "#     'Stimmen': 'Votes',\n",
    "#     'Preis': 'Price',\n",
    "#     'Year': 'Year',\n",
    "#     'Available Budget': 'Budget for this year',\n",
    "#     'Approved': 'Approved'\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Drop unnecessary columns if any remain\n",
    "# budget_jena_df = budget_jena_df.loc[:, ~budget_jena_df.columns.str.contains('Verf√ºgbareBudgetmittel', na=False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments Scraper\n",
    "\n",
    "This scraper extracts comments for all projects from the `all_projects_df` DataFrame and organizes them into a structured DataFrame:\n",
    "\n",
    "1. **`df_comments`**: Contains all comments associated with projects.\n",
    "   - Columns:\n",
    "     - `URL`: The URL of the project the comment is associated with.\n",
    "     - `Project`: The title of the project the comment is associated with.\n",
    "     - `City`: The city the project belongs to (extracted from the URL).\n",
    "     - `Text`: The content of the comment.\n",
    "     - `Username`: The name of the user who posted the comment.\n",
    "     - `Date`: The date the comment was posted.\n",
    "     - `Likes`: The number of likes the comment received.\n",
    "     - `Dislikes`: The number of dislikes the comment received.\n",
    "     - `Total Votes`: The total votes (likes + dislikes) the comment received.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good scraper for comments (748 entries)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Updated function to extract comments from a single page\n",
    "def extract_comments_from_page(soup):\n",
    "    comments_data = []\n",
    "    comments_section = soup.find_all('div', class_='comment small-12')\n",
    "    \n",
    "    for comment in comments_section:\n",
    "        # Extract comment text\n",
    "        comment_text = comment.find('p').get_text(strip=True) if comment.find('p') else None\n",
    "        \n",
    "        # Extract username\n",
    "        username_tag = comment.find('span', class_='user-name')\n",
    "        username = username_tag.get_text(strip=True) if username_tag else None\n",
    "        \n",
    "        # Extract date\n",
    "        date_tag = comment.find('div', class_='comment-info').find_all('a')[-1]\n",
    "        date = date_tag.get_text(strip=True) if date_tag else None\n",
    "\n",
    "        \n",
    "        # Extract likes and dislikes (clean and convert to integer)\n",
    "        likes_tag = comment.find('span', class_='in-favor')\n",
    "        likes = int(re.sub(r'\\D', '', likes_tag.get_text(strip=True))) if likes_tag else 0\n",
    "        \n",
    "        dislikes_tag = comment.find('span', class_='against')\n",
    "        dislikes = int(re.sub(r'\\D', '', dislikes_tag.get_text(strip=True))) if dislikes_tag else 0\n",
    "        \n",
    "        # Extract total votes (clean and convert to integer)\n",
    "        total_votes = likes + dislikes\n",
    "        \n",
    "        comments_data.append({\n",
    "            'Text': comment_text,\n",
    "            'Username': username,\n",
    "            'Date': date,\n",
    "            'Likes': likes,\n",
    "            'Dislikes': dislikes,\n",
    "            'Total Votes': total_votes\n",
    "        })\n",
    "    return comments_data\n",
    "\n",
    "\n",
    "# Function to extract city name from the base URL\n",
    "def extract_city_name(base_url):\n",
    "    # Words to remove from the city name\n",
    "    remove_words = ['mitmachen', 'Mitmachen', 'mitwirken', 'Smarte', 'region', 'unser', 'mitgestalten', 'gestalten', 'machmit', 'dialog', 'consul', 'www', 'de', 'https', 'com']\n",
    "\n",
    "    # Split the URL into parts (by '.' or '/')\n",
    "    parts = base_url.replace('https://', '').replace('http://', '').split('.')\n",
    "    all_parts = [part.split('/')[0] for part in parts]  # Handle cases where \"/\" exists after domain\n",
    "\n",
    "    # Remove known unwanted words and empty strings\n",
    "    filtered_parts = [part for part in all_parts if part.lower() not in remove_words and part]\n",
    "\n",
    "    # Return the first relevant part (assumes city name is left after filtering)\n",
    "    city = filtered_parts[0].replace('-', ' ').capitalize() if filtered_parts else \"Unknown\"\n",
    "\n",
    "    # Remove unwanted words from city name\n",
    "    for word in remove_words:\n",
    "        city = city.replace(word, '')\n",
    "    \n",
    "\n",
    "    return city.strip().capitalize()\n",
    "\n",
    "\n",
    "# Scrape all comments across pages (pagination logic remains the same)\n",
    "def scrape_all_comments(base_url):\n",
    "    comments = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        paginated_url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        response = requests.get(paginated_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load page {page} for URL: {base_url}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        comments_on_page = extract_comments_from_page(soup)\n",
    "        \n",
    "        if not comments_on_page:  # Stop if no comments on the page\n",
    "            break\n",
    "        \n",
    "        comments.extend(comments_on_page)\n",
    "        page += 1\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Function to scrape the main content and comments for each URL\n",
    "def scrape_content_and_comments(urls):\n",
    "    data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load URL: {url}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Scrape main content\n",
    "        title = soup.find('title').get_text(strip=True) if soup.find('title') else None\n",
    "        content_div = soup.find('div', class_='flex-layout')\n",
    "        content = content_div.get_text(strip=True) if content_div else None\n",
    "        \n",
    "        # Scrape comments\n",
    "        comments = scrape_all_comments(url)\n",
    "        \n",
    "        data.append({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Content': content,\n",
    "            'Comments': comments\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Scrape comments for all project URLs from all_projects_df\n",
    "urls = all_projects_df['Project URL'].tolist()  # Use the 'Project URL' column from all_projects_df\n",
    "scraped_data = scrape_content_and_comments(urls)\n",
    "\n",
    "# Create structured DataFrame for comments\n",
    "comments_data = []\n",
    "for item in scraped_data:\n",
    "    for comment in item['Comments']:\n",
    "        comment['URL'] = item['URL']  # Link comment to the project URL\n",
    "        # Extract city name from URL\n",
    "        city = extract_city_name(item['URL'])\n",
    "        comment['City'] = city\n",
    "        comments_data.append(comment)\n",
    "\n",
    "# Create the comments DataFrame\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Create a mapping from URL to Project Title\n",
    "url_to_title = all_projects_df.set_index('Project URL')['Project Title'].to_dict()\n",
    "\n",
    "# Add a 'Project' column to df_comments using the mapping\n",
    "df_comments['Project'] = df_comments['URL'].map(url_to_title)\n",
    "df_comments = df_comments[['URL', 'Project', 'City'] + [col for col in df_comments.columns if col not in ['URL', 'Project', 'City']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Username\n",
       "Lars L√∂w           34\n",
       "klaus.kleiner77    32\n",
       "Der,wo             31\n",
       "PM                 18\n",
       "Klaus.kleiner77    16\n",
       "                   ..\n",
       "Julius Kuhn         1\n",
       "Juliane88           1\n",
       "Juliane Fuchs       1\n",
       "Julian Sing         1\n",
       "üêô                   1\n",
       "Name: count, Length: 387, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cleaaning and structuring for Sieburg (review if it's needed) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Enhanced function to extract all logical parts, including \"Unterst√ºtzer*innen\"\n",
    "# def extract_full_data_with_supporters(content):\n",
    "#     # Extract title (everything before the first date)\n",
    "#     title_match = re.search(r'^(.*?)(\\r|\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "#     title = title_match.group(1).strip() if title_match else None\n",
    "\n",
    "#     # Extract date\n",
    "#     date_match = re.search(r'\\d{1,2}\\.\\s\\w+\\s\\d{4}', content)\n",
    "#     date = date_match.group(0) if date_match else None\n",
    "\n",
    "#     # Extract comments count\n",
    "#     comments_match = re.search(r'(\\d+)\\sKommentare', content)\n",
    "#     comments = int(comments_match.group(1)) if comments_match else 0\n",
    "\n",
    "#     # Extract tags (sections with numbers or + signs)\n",
    "#     tags_match = re.findall(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "#     tags = ', '.join(tags_match) if tags_match else None\n",
    "\n",
    "#     # Extract description (everything after \"Geselliges Beisammensein\" or similar patterns)\n",
    "#     description_start = re.search(r'(Geselliges Beisammensein|Angebotslandkarte)', content)\n",
    "#     description = content[description_start.start():].strip() if description_start else None\n",
    "\n",
    "#     # Extract username\n",
    "#     username_match = re.search(r'(\\w+\\s\\w+|Beigetreten am:.*?\\d{4})', content)\n",
    "#     username = username_match.group(1).split('Beigetreten am:')[0].strip() if username_match else None\n",
    "\n",
    "#     # Extract Vorschl√§ge count\n",
    "#     vorschlaege_match = re.search(r'Vorschl√§ge(\\d+)', content)\n",
    "#     vorschlaege = int(vorschlaege_match.group(1)) if vorschlaege_match else 0\n",
    "\n",
    "#     # Extract Konto verification status\n",
    "#     konto_match = re.search(r'(Konto\\s(verifiziert|ist nicht verifiziert))', content)\n",
    "#     konto_status = konto_match.group(2) if konto_match else None\n",
    "\n",
    "#     # # Extract registration date\n",
    "#     # registration_match = re.search(r'Beigetreten am:\\s(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "#     # registration_date = registration_match.group(1) if registration_match else None\n",
    "\n",
    "#     # Extract number of Unterst√ºtzer*innen\n",
    "#     supporters_match = re.search(r'(\\d+)\\sUnterst√ºtzer\\*in', content)\n",
    "#     supporters = int(supporters_match.group(1)) if supporters_match else 0\n",
    "\n",
    "#     return title, date, comments, tags, description, username, vorschlaege, konto_status, supporters\n",
    "\n",
    "# # Apply the enhanced function to the DataFrame and create new columns\n",
    "# df_sieburg[['Title', 'Date', 'Comments', 'Tags', 'Description', 'Username', 'Vorschl√§ge', 'Konto Status', 'Supporters']] = df_sieburg['Content'].apply(\n",
    "#     lambda x: pd.Series(extract_full_data_with_supporters(x))\n",
    "# )\n",
    "\n",
    "\n",
    "# # Function to clean description considering keywords, numeric patterns, and refined starting logic\n",
    "# def clean_description_advanced(content):\n",
    "#     # Define keywords that mark the beginning of the description\n",
    "#     keywords = [\n",
    "#         'Geselliges Beisammensein', 'Natur', 'Hilfe & Beratung', 'Bildung', \n",
    "#         'Musik', 'Bewegung', 'Glaube', 'Kulinarisches', 'Kunst & Kultur', 'Sonstiges',\n",
    "#     ]\n",
    "    \n",
    "#     # Check for keywords first\n",
    "#     for keyword in keywords:\n",
    "#         if keyword in content:\n",
    "#             start_idx = content.find(keyword) + len(keyword)\n",
    "#             description = content[start_idx:].strip()\n",
    "#             description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#             return description\n",
    "\n",
    "#     # If no keyword is found, check for numeric patterns like \"18-24, 25-49, etc.\"\n",
    "#     numeric_pattern = re.search(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "#     if numeric_pattern:\n",
    "#         start_idx = numeric_pattern.end()\n",
    "#         description = content[start_idx:].strip()\n",
    "#         description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#         return description\n",
    "\n",
    "#     # As a fallback, find the first capital letter, quote, or digit to mark the start\n",
    "#     fallback_match = re.search(r'[A-Z\"0-9]', content)\n",
    "#     if fallback_match:\n",
    "#         start_idx = fallback_match.start()\n",
    "#         description = content[start_idx:].strip()\n",
    "#         description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "#         return description\n",
    "\n",
    "#     # If nothing works, return the content as is\n",
    "#     return content\n",
    "\n",
    "# # Apply the advanced cleaning function to the Description column\n",
    "# df_sieburg['Description'] = df_sieburg['Content'].apply(clean_description_advanced)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
