{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def content_scraper(soup, links, identifier):\n",
    "    link_and_data = {}\n",
    "\n",
    "    for link in links:  # Iterate over all links\n",
    "        response = requests.get(link)  # Use the current link\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Handle scraping logic based on the identifier\n",
    "            match identifier:\n",
    "                case 'jena':\n",
    "                    title = soup.find_all('title')\n",
    "                    content = soup.find_all('div', class_='flex-layout')\n",
    "\n",
    "                    # Extract title and content if found\n",
    "                    text_content = (\n",
    "                        title[0].get_text(strip=True) + ': ' + content[0].get_text(strip=True)\n",
    "                        if title and content else \"No content found\"\n",
    "                    )\n",
    "                case 'siegburg':\n",
    "                    divs = soup.find_all('div', class_='flex-layout')\n",
    "                    content = divs[1] if len(divs) > 1 else None\n",
    "\n",
    "                    # Extract content if found\n",
    "                    text_content = content.get_text(strip=True) if content else \"No content found\"\n",
    "                case _:\n",
    "                    text_content = \"Wrong identifier\"\n",
    "\n",
    "            link_and_data[link] = text_content\n",
    "        else:\n",
    "            # Handle failed requests\n",
    "            link_and_data[link] = f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "    return link_and_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siegburg_data(soup):\n",
    "    links = []\n",
    "\n",
    "    a_tags = soup.find_all('a')\n",
    "\n",
    "    for tag in a_tags:\n",
    "        href = tag.get('href')\n",
    "        \n",
    "        if href and '/proposals/' in href and not 'new' in href:\n",
    "            link = 'https://mitmachen.siegburg.de' + href\n",
    "    \n",
    "            if not link in links:\n",
    "                links.append(link)\n",
    "    \n",
    "    link_and_content = content_scraper(soup, links, 'siegburg')\n",
    "\n",
    "    return link_and_content\n",
    "\n",
    "\n",
    "def jena_data(soup):\n",
    "    links = []\n",
    "\n",
    "    a_tags = soup.find_all('a',  class_='resource-item--title')\n",
    "\n",
    "    for tag in a_tags:\n",
    "        href = tag.get('href')\n",
    "        \n",
    "        if href:\n",
    "            link = 'https://mitmachen.jena.de' + href\n",
    "            \n",
    "            if not link in links:\n",
    "                links.append(link)\n",
    "    \n",
    "    link_and_content = content_scraper(soup, links, 'jena')\n",
    "\n",
    "    return link_and_content\n",
    "\n",
    "\n",
    "def wurzburg_data(soup):\n",
    "    links = []\n",
    "\n",
    "    # Find all resource links for Würzburg (similar to Jena)\n",
    "    a_tags = soup.find_all('a', class_='resource-item--title')\n",
    "\n",
    "    for tag in a_tags:\n",
    "        href = tag.get('href')\n",
    "\n",
    "        if href:\n",
    "            link = 'https://wuerzburg-mitmachen.de' + href\n",
    "\n",
    "            if link not in links:\n",
    "                links.append(link)\n",
    "\n",
    "    # Scrape content from each link\n",
    "    link_and_content = content_scraper(soup, links, 'jena')  # Reusing 'jena' scraping logic as the structure is the same\n",
    "\n",
    "    return link_and_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://mitmachen.siegburg.de/angebotslandkarte',\n",
    "    'https://mitmachen.jena.de/projekts',\n",
    "    'https://wuerzburg-mitmachen.de/projekts'\n",
    "]\n",
    "\n",
    "\n",
    "def def_42(urls):\n",
    "    urls_and_data = {}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        link_and_content = scraper(soup, url)\n",
    "        urls_and_data[url] = link_and_content\n",
    "\n",
    "    return urls_and_data\n",
    "\n",
    "\n",
    "def scraper(soup, url):\n",
    "    if 'siegburg' in url:\n",
    "        return siegburg_data(soup)\n",
    "    elif 'jena' in url:\n",
    "        return jena_data(soup)\n",
    "    elif 'wuerzburg' in url:  # Add handling for Würzburg\n",
    "        return wurzburg_data(soup)\n",
    "\n",
    "    return \"No scraper defined for this URL\"\n",
    "\n",
    "\n",
    "link_and_data = def_42(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate scraper for Wurzburg (when you have to check \"inner\" proposals inside of a project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Updated function to extract proposals from a project page\n",
    "def extract_proposals(soup, base_url):\n",
    "    proposals = []\n",
    "    proposal_items = soup.find_all('div', class_='resource-item proposal-list-item')\n",
    "\n",
    "    for proposal in proposal_items:\n",
    "        # Extract title\n",
    "        title_tag = proposal.find('a', class_='resource-item--title')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "        # Extract URL\n",
    "        url = base_url + title_tag['href'] if title_tag and 'href' in title_tag.attrs else None\n",
    "\n",
    "        # Extract description\n",
    "        description_tag = proposal.find('div', class_='resource-item--description')\n",
    "        description = description_tag.get_text(strip=True) if description_tag else None\n",
    "\n",
    "        # Extract author/username\n",
    "        author_tag = proposal.find('a', class_='resource-item--author')\n",
    "        author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "        # Extract number of comments\n",
    "        comments_tag = proposal.find('span', class_='comments')\n",
    "        comments = int(comments_tag.get_text(strip=True).split()[0]) if comments_tag else 0\n",
    "\n",
    "        # Extract number of supporters\n",
    "        supporters_tag = proposal.find('span', class_='total-supports')\n",
    "        supporters = int(supporters_tag.get_text(strip=True).split()[0]) if supporters_tag else 0\n",
    "\n",
    "        # Extract parent project\n",
    "        project_tag = proposal.find('a', class_='breadcrumbs-item')\n",
    "        proposed_for_project = project_tag.get_text(strip=True) if project_tag else None\n",
    "\n",
    "        proposals.append({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Proposed for Project': proposed_for_project,\n",
    "            'Description': description,\n",
    "            'Author': author,\n",
    "            'Comments': comments,\n",
    "            'Supporters': supporters,\n",
    "        })\n",
    "    return proposals\n",
    "\n",
    "\n",
    "\n",
    "# Function to scrape a single project page and check for proposals\n",
    "def scrape_project_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load project page: {url}\")\n",
    "        return None, []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract project title\n",
    "    title_tag = soup.find('title')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "    # Extract project description\n",
    "    content_div = soup.find('div', class_='flex-layout')\n",
    "    description = content_div.get_text(strip=True) if content_div else None\n",
    "\n",
    "    # Check for proposals in the page\n",
    "    proposals = extract_proposals(soup, base_url=url)\n",
    "\n",
    "    return {\n",
    "        'Project URL': url,\n",
    "        'Project Title': title,\n",
    "        'Project Description': description,\n",
    "        'Proposal Count': len(proposals)\n",
    "    }, proposals\n",
    "\n",
    "# Main scraping function for Wurzburg projects\n",
    "def scrape_wurzburg_projects(main_url):\n",
    "    response = requests.get(main_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load main projects page: {main_url}\")\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all project links\n",
    "    links = soup.find_all('a', class_='resource-item--title')\n",
    "    project_links = ['https://wuerzburg-mitmachen.de' + link['href'] for link in links if 'href' in link.attrs]\n",
    "\n",
    "    projects = []\n",
    "    all_proposals = []\n",
    "\n",
    "    for project_url in project_links:\n",
    "        project_data, proposals = scrape_project_page(project_url)\n",
    "        if project_data:\n",
    "            projects.append(project_data)\n",
    "            all_proposals.extend(proposals)\n",
    "\n",
    "    return pd.DataFrame(projects), pd.DataFrame(all_proposals)\n",
    "\n",
    "# Run the scraper for Wurzburg\n",
    "wurzburg_projects_url = 'https://wuerzburg-mitmachen.de/projekts'\n",
    "df_wurzburg_projects, df_wurzburg_proposals = scrape_wurzburg_projects(wurzburg_projects_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project URL</th>\n",
       "      <th>Project Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Proposal Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/mobilitaetsplan</td>\n",
       "      <td>Mobilitätsplan 2040</td>\n",
       "      <td>Mobilitätsplan 2040 für die Stadt Würzburg: Jetzt mitmachen!Aktuell erstellt die Stadt Würzburg ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftsregion</td>\n",
       "      <td>Zukunftsregion Würzburg</td>\n",
       "      <td>Zukunftsregion Würzburg: Jetzt aktiv mitgestalten!Die Stadt und der Landkreis Würzburg wollen ih...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftskonzepte-fuer-die-innenstadt</td>\n",
       "      <td>Zukunftskonzepte für die Innenstadt</td>\n",
       "      <td>Wie soll die Würzburger Innenstadt von morgen aussehen? Was wünschen sich Bürger:innen, Einzelhä...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/klimaanpassung</td>\n",
       "      <td>Klimaanpassung</td>\n",
       "      <td>Klimaanpassungsstrategie für die Stadt Würzburg: Jetzt mitmachen!Würzburg - Seit Anfang 2024 era...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/europawahl-2024</td>\n",
       "      <td>Europawahl 2024</td>\n",
       "      <td>Würzburg bereitet sich auf Europawahl vor: Engagement für Erstwählerinnen und ErstwählerWürzburg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Project URL  \\\n",
       "0                       https://wuerzburg-mitmachen.de/mobilitaetsplan   \n",
       "1                        https://wuerzburg-mitmachen.de/zukunftsregion   \n",
       "2  https://wuerzburg-mitmachen.de/zukunftskonzepte-fuer-die-innenstadt   \n",
       "3                        https://wuerzburg-mitmachen.de/klimaanpassung   \n",
       "4                       https://wuerzburg-mitmachen.de/europawahl-2024   \n",
       "\n",
       "                         Project Title  \\\n",
       "0                  Mobilitätsplan 2040   \n",
       "1              Zukunftsregion Würzburg   \n",
       "2  Zukunftskonzepte für die Innenstadt   \n",
       "3                       Klimaanpassung   \n",
       "4                      Europawahl 2024   \n",
       "\n",
       "                                                                                   Project Description  \\\n",
       "0  Mobilitätsplan 2040 für die Stadt Würzburg: Jetzt mitmachen!Aktuell erstellt die Stadt Würzburg ...   \n",
       "1  Zukunftsregion Würzburg: Jetzt aktiv mitgestalten!Die Stadt und der Landkreis Würzburg wollen ih...   \n",
       "2  Wie soll die Würzburger Innenstadt von morgen aussehen? Was wünschen sich Bürger:innen, Einzelhä...   \n",
       "3  Klimaanpassungsstrategie für die Stadt Würzburg: Jetzt mitmachen!Würzburg - Seit Anfang 2024 era...   \n",
       "4  Würzburg bereitet sich auf Europawahl vor: Engagement für Erstwählerinnen und ErstwählerWürzburg...   \n",
       "\n",
       "   Proposal Count  \n",
       "0               0  \n",
       "1               0  \n",
       "2              24  \n",
       "3              14  \n",
       "4               0  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wurzburg_projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Separate URLs and content for Siegburg and Jena\n",
    "siegburg_data = {k: v for k, v in link_and_data.items() if \"siegburg\" in k}\n",
    "jena_data = {k: v for k, v in link_and_data.items() if \"jena\" in k}\n",
    "# wurzburg_data = {k: v for k, v in link_and_data.items() if \"wuerzburg\" in k}\n",
    "\n",
    "# Create DataFrames\n",
    "df_sieburg = pd.DataFrame(list(siegburg_data.items()), columns=[\"URL\", \"Content\"])\n",
    "df_jena = pd.DataFrame(list(jena_data.items()), columns=[\"URL\", \"Content\"])\n",
    "# df_wurzburg = pd.DataFrame(list(wurzburg_data.items()), columns=[\"URL\", \"Content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jena = pd.DataFrame(df_jena['Content'][0].items(), columns=[\"URL\", \"Content\"])\n",
    "df_sieburg = pd.DataFrame(df_sieburg['Content'][0].items(), columns=[\"URL\", \"Content\"])\n",
    "# df_wurzburg = pd.DataFrame(df_wurzburg['Content'][0].items(), columns=[\"URL\", \"Content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and structurising Sieburg Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Enhanced function to extract all logical parts, including \"Unterstützer*innen\"\n",
    "def extract_full_data_with_supporters(content):\n",
    "    # Extract title (everything before the first date)\n",
    "    title_match = re.search(r'^(.*?)(\\r|\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "    title = title_match.group(1).strip() if title_match else None\n",
    "\n",
    "    # Extract date\n",
    "    date_match = re.search(r'\\d{1,2}\\.\\s\\w+\\s\\d{4}', content)\n",
    "    date = date_match.group(0) if date_match else None\n",
    "\n",
    "    # Extract comments count\n",
    "    comments_match = re.search(r'(\\d+)\\sKommentare', content)\n",
    "    comments = int(comments_match.group(1)) if comments_match else 0\n",
    "\n",
    "    # Extract tags (sections with numbers or + signs)\n",
    "    tags_match = re.findall(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "    tags = ', '.join(tags_match) if tags_match else None\n",
    "\n",
    "    # Extract description (everything after \"Geselliges Beisammensein\" or similar patterns)\n",
    "    description_start = re.search(r'(Geselliges Beisammensein|Angebotslandkarte)', content)\n",
    "    description = content[description_start.start():].strip() if description_start else None\n",
    "\n",
    "    # Extract username\n",
    "    username_match = re.search(r'(\\w+\\s\\w+|Beigetreten am:.*?\\d{4})', content)\n",
    "    username = username_match.group(1).split('Beigetreten am:')[0].strip() if username_match else None\n",
    "\n",
    "    # Extract Vorschläge count\n",
    "    vorschlaege_match = re.search(r'Vorschläge(\\d+)', content)\n",
    "    vorschlaege = int(vorschlaege_match.group(1)) if vorschlaege_match else 0\n",
    "\n",
    "    # Extract Konto verification status\n",
    "    konto_match = re.search(r'(Konto\\s(verifiziert|ist nicht verifiziert))', content)\n",
    "    konto_status = konto_match.group(2) if konto_match else None\n",
    "\n",
    "    # # Extract registration date\n",
    "    # registration_match = re.search(r'Beigetreten am:\\s(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "    # registration_date = registration_match.group(1) if registration_match else None\n",
    "\n",
    "    # Extract number of Unterstützer*innen\n",
    "    supporters_match = re.search(r'(\\d+)\\sUnterstützer\\*in', content)\n",
    "    supporters = int(supporters_match.group(1)) if supporters_match else 0\n",
    "\n",
    "    return title, date, comments, tags, description, username, vorschlaege, konto_status, supporters\n",
    "\n",
    "# Apply the enhanced function to the DataFrame and create new columns\n",
    "df_sieburg[['Title', 'Date', 'Comments', 'Tags', 'Description', 'Username', 'Vorschläge', 'Konto Status', 'Supporters']] = df_sieburg['Content'].apply(\n",
    "    lambda x: pd.Series(extract_full_data_with_supporters(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean description considering keywords, numeric patterns, and refined starting logic\n",
    "def clean_description_advanced(content):\n",
    "    # Define keywords that mark the beginning of the description\n",
    "    keywords = [\n",
    "        'Geselliges Beisammensein', 'Natur', 'Hilfe & Beratung', 'Bildung', \n",
    "        'Musik', 'Bewegung', 'Glaube', 'Kulinarisches', 'Kunst & Kultur', 'Sonstiges',\n",
    "    ]\n",
    "    \n",
    "    # Check for keywords first\n",
    "    for keyword in keywords:\n",
    "        if keyword in content:\n",
    "            start_idx = content.find(keyword) + len(keyword)\n",
    "            description = content[start_idx:].strip()\n",
    "            description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "            return description\n",
    "\n",
    "    # If no keyword is found, check for numeric patterns like \"18-24, 25-49, etc.\"\n",
    "    numeric_pattern = re.search(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "    if numeric_pattern:\n",
    "        start_idx = numeric_pattern.end()\n",
    "        description = content[start_idx:].strip()\n",
    "        description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "        return description\n",
    "\n",
    "    # As a fallback, find the first capital letter, quote, or digit to mark the start\n",
    "    fallback_match = re.search(r'[A-Z\"0-9]', content)\n",
    "    if fallback_match:\n",
    "        start_idx = fallback_match.start()\n",
    "        description = content[start_idx:].strip()\n",
    "        description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "        return description\n",
    "\n",
    "    # If nothing works, return the content as is\n",
    "    return content\n",
    "\n",
    "# Apply the advanced cleaning function to the Description column\n",
    "df_sieburg['Description'] = df_sieburg['Content'].apply(clean_description_advanced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and structurising Jena Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized function to process all entries in df_jena['Content']\n",
    "def process_all_jena_entries(df_jena):\n",
    "    # Generalized function for extracting data\n",
    "    def extract_jena_data(content, soup):\n",
    "        # Extract Title\n",
    "        title_match = re.search(r'^(.*?):', content)\n",
    "        title = title_match.group(1).strip() if title_match else None\n",
    "\n",
    "        # Extract Description\n",
    "        description_match = re.search(r':\\s*(.*?)\\n⭐', content, re.DOTALL)\n",
    "        description = description_match.group(1).strip() if description_match else None\n",
    "\n",
    "        # Extract Project Start Date\n",
    "        start_date_match = re.search(r'Projektstart\\s*(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "        start_date = start_date_match.group(1) if start_date_match else None\n",
    "\n",
    "        # Extract Username\n",
    "        username_match = re.search(r'@(\\w+)', content)\n",
    "        username = username_match.group(1) if username_match else None\n",
    "\n",
    "        # Extract Tags\n",
    "        tags_match = re.findall(r'#(\\w+)', content)\n",
    "        tags = ', '.join(tags_match) if tags_match else None\n",
    "\n",
    "        # Extract Supporters Count\n",
    "        supporters_match = re.search(r'(\\d+)\\sUnterstützer\\*in', content)\n",
    "        supporters = int(supporters_match.group(1)) if supporters_match else None\n",
    "\n",
    "        # Extract Number of Discussions\n",
    "        discussions_match = re.search(r'DiskussionenAbgeschlossen\\sam\\s(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "        discussions = discussions_match.group(1) if discussions_match else None\n",
    "\n",
    "        # Extract Number of Comments from the h4 tag\n",
    "        comments_tag = soup.find('h4', text=re.compile(r'Kommentare'))\n",
    "        comments_count = int(re.search(r'\\((\\d+)\\)', comments_tag.get_text(strip=True)).group(1)) if comments_tag else 0\n",
    "\n",
    "        return {\n",
    "            \"Title\": title,\n",
    "            \"Description\": description,\n",
    "            \"Project Start Date\": start_date,\n",
    "            \"Username\": username,\n",
    "            \"Tags\": tags,\n",
    "            \"Supporters\": supporters,\n",
    "            \"Discussions\": discussions,\n",
    "            \"Comments\": comments_count\n",
    "        }\n",
    "\n",
    "\n",
    "    extracted_data = []\n",
    "    for _, row in df_jena.iterrows():\n",
    "        response = requests.get(row[\"URL\"])\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load URL: {row['URL']}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        data = extract_jena_data(row[\"Content\"], soup)\n",
    "        extracted_data.append(data)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df_jena_cleaned = pd.DataFrame([{\n",
    "        \"URL\": row[\"URL\"],\n",
    "        \"Title\": data[\"Title\"],\n",
    "        # \"Description\": data[\"Description\"],\n",
    "        \"Project Start Date\": data[\"Project Start Date\"],\n",
    "        # \"Username\": data[\"Username\"],\n",
    "        # \"Tags\": data[\"Tags\"],\n",
    "        \"Supporters\": data[\"Supporters\"],\n",
    "        \"Discussions\": data[\"Discussions\"],\n",
    "        \"Comments\": data[\"Comments\"]\n",
    "    } for row, data in zip(df_jena.to_dict('records'), extracted_data)])\n",
    "\n",
    "    return df_jena_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rusel\\AppData\\Local\\Temp\\ipykernel_20416\\120626955.py:34: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  comments_tag = soup.find('h4', text=re.compile(r'Kommentare'))\n",
      "C:\\Users\\Rusel\\AppData\\Local\\Temp\\ipykernel_20416\\120626955.py:34: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  comments_tag = soup.find('h4', text=re.compile(r'Kommentare'))\n"
     ]
    }
   ],
   "source": [
    "df_jena_cleaned = process_all_jena_entries(df_jena)\n",
    "df_wurzburg_cleaned = process_all_jena_entries(df_wurzburg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Project Start Date</th>\n",
       "      <th>Supporters</th>\n",
       "      <th>Discussions</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/mobilitaetsplan</td>\n",
       "      <td>Mobilitätsplan 2040</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftsregion</td>\n",
       "      <td>Zukunftsregion Würzburg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/zukunftskonzepte-fuer-die-innenstadt</td>\n",
       "      <td>Zukunftskonzepte für die Innenstadt</td>\n",
       "      <td>13. Mai 2024</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/klimaanpassung</td>\n",
       "      <td>Klimaanpassung</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/europawahl-2024</td>\n",
       "      <td>Europawahl 2024</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/umfrage-zum-neuen-buerger-und-stadtteilzentrum-lindleinsmuehle</td>\n",
       "      <td>Bürgerbeteiligung in der Lindleinsmühle</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/freizeitgelaende-katzenbergtunnel</td>\n",
       "      <td>Freizeitgelände Katzenbergtunnel</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/laermaktionsplan</td>\n",
       "      <td>Lärmaktionsplan</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/neugestaltung-des-mainufers-heidingsfeld</td>\n",
       "      <td>Neugestaltung des Mainufers Heidingsfeld</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/vu-suedlicher-bischofshut</td>\n",
       "      <td>VU Südlicher Bischofshut</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/katzenbergtunnel</td>\n",
       "      <td>Auf dem Katzenbergtunnel entsteht ein neues Freizeitgelände</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/integriertes-staedtebauliches-entwicklungskonzept-grombuehl</td>\n",
       "      <td>Integriertes Städtebauliches Entwicklungskonzept Grombühl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/integriertes-staedtebauliches-entwicklungskonzept-grombuehl-2</td>\n",
       "      <td>Integriertes Städtebauliches Entwicklungskonzept Grombühl - Phase 2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/strassenbahnlinie-hubland</td>\n",
       "      <td>Straßenbahnlinie Hubland</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/befragung-hitze</td>\n",
       "      <td>Sommer-Sonne-Hitzeschutz</td>\n",
       "      <td>17. Juli 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/smart-city-hub</td>\n",
       "      <td>Smart City Hub</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/wue-app</td>\n",
       "      <td>Wue App</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/mach-mit-</td>\n",
       "      <td>Mach mit</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/heldenhelfer-</td>\n",
       "      <td>Heldenhelfer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/heimat-forum</td>\n",
       "      <td>Heimatforum</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://wuerzburg-mitmachen.de/wie-gehts-</td>\n",
       "      <td>Wie gehts</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              URL  \\\n",
       "0                                                  https://wuerzburg-mitmachen.de/mobilitaetsplan   \n",
       "1                                                   https://wuerzburg-mitmachen.de/zukunftsregion   \n",
       "2                             https://wuerzburg-mitmachen.de/zukunftskonzepte-fuer-die-innenstadt   \n",
       "3                                                   https://wuerzburg-mitmachen.de/klimaanpassung   \n",
       "4                                                  https://wuerzburg-mitmachen.de/europawahl-2024   \n",
       "5   https://wuerzburg-mitmachen.de/umfrage-zum-neuen-buerger-und-stadtteilzentrum-lindleinsmuehle   \n",
       "6                                https://wuerzburg-mitmachen.de/freizeitgelaende-katzenbergtunnel   \n",
       "7                                                 https://wuerzburg-mitmachen.de/laermaktionsplan   \n",
       "8                         https://wuerzburg-mitmachen.de/neugestaltung-des-mainufers-heidingsfeld   \n",
       "9                                        https://wuerzburg-mitmachen.de/vu-suedlicher-bischofshut   \n",
       "10                                                https://wuerzburg-mitmachen.de/katzenbergtunnel   \n",
       "11     https://wuerzburg-mitmachen.de/integriertes-staedtebauliches-entwicklungskonzept-grombuehl   \n",
       "12   https://wuerzburg-mitmachen.de/integriertes-staedtebauliches-entwicklungskonzept-grombuehl-2   \n",
       "13                                       https://wuerzburg-mitmachen.de/strassenbahnlinie-hubland   \n",
       "14                                                 https://wuerzburg-mitmachen.de/befragung-hitze   \n",
       "15                                                  https://wuerzburg-mitmachen.de/smart-city-hub   \n",
       "16                                                         https://wuerzburg-mitmachen.de/wue-app   \n",
       "17                                                       https://wuerzburg-mitmachen.de/mach-mit-   \n",
       "18                                                   https://wuerzburg-mitmachen.de/heldenhelfer-   \n",
       "19                                                    https://wuerzburg-mitmachen.de/heimat-forum   \n",
       "20                                                      https://wuerzburg-mitmachen.de/wie-gehts-   \n",
       "\n",
       "                                                                  Title  \\\n",
       "0                                                   Mobilitätsplan 2040   \n",
       "1                                               Zukunftsregion Würzburg   \n",
       "2                                   Zukunftskonzepte für die Innenstadt   \n",
       "3                                                        Klimaanpassung   \n",
       "4                                                       Europawahl 2024   \n",
       "5                               Bürgerbeteiligung in der Lindleinsmühle   \n",
       "6                                      Freizeitgelände Katzenbergtunnel   \n",
       "7                                                       Lärmaktionsplan   \n",
       "8                              Neugestaltung des Mainufers Heidingsfeld   \n",
       "9                                              VU Südlicher Bischofshut   \n",
       "10          Auf dem Katzenbergtunnel entsteht ein neues Freizeitgelände   \n",
       "11            Integriertes Städtebauliches Entwicklungskonzept Grombühl   \n",
       "12  Integriertes Städtebauliches Entwicklungskonzept Grombühl - Phase 2   \n",
       "13                                             Straßenbahnlinie Hubland   \n",
       "14                                             Sommer-Sonne-Hitzeschutz   \n",
       "15                                                       Smart City Hub   \n",
       "16                                                              Wue App   \n",
       "17                                                             Mach mit   \n",
       "18                                                         Heldenhelfer   \n",
       "19                                                          Heimatforum   \n",
       "20                                                            Wie gehts   \n",
       "\n",
       "   Project Start Date Supporters Discussions  Comments  \n",
       "0                None       None        None         0  \n",
       "1                None       None        None         0  \n",
       "2        13. Mai 2024       None        None         0  \n",
       "3                None       None        None         0  \n",
       "4                None       None        None         0  \n",
       "5                None       None        None         0  \n",
       "6                None       None        None         0  \n",
       "7                None       None        None         0  \n",
       "8                None       None        None         0  \n",
       "9                None       None        None         0  \n",
       "10               None       None        None         0  \n",
       "11               None       None        None         0  \n",
       "12               None       None        None         0  \n",
       "13               None       None        None         0  \n",
       "14      17. Juli 2023       None        None         0  \n",
       "15               None       None        None         0  \n",
       "16               None       None        None         0  \n",
       "17               None       None        None         0  \n",
       "18               None       None        None         0  \n",
       "19               None       None        None         0  \n",
       "20               None       None        None         0  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wurzburg_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BurgerBudgets in Jena (2024, 23, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for the budgets\n",
    "budget_urls = {\n",
    "    2024: \"https://mitmachen.jena.de/buergerbudget\",\n",
    "    2023: \"https://mitmachen.jena.de/buergerbudget-2023\",\n",
    "    2022: \"https://mitmachen.jena.de/buergerbudget-2022\"\n",
    "}\n",
    "\n",
    "# Updated function to scrape and clean a budget table for a given year\n",
    "def scrape_and_clean_budget_table(url, year):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load URL: {url}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', id='budget-investments-compatible')  # Locate the table by its ID\n",
    "    \n",
    "    if not table:\n",
    "        print(f\"No table found for URL: {url}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the total available budget for the year (last <th> in <thead>)\n",
    "    available_budget_tag = table.find('thead').find_all('th')[-1]  # Find the last <th>\n",
    "    available_budget = (\n",
    "        float(re.sub(r'[^\\d.]', '', available_budget_tag.get_text(strip=True))) * 1000\n",
    "        if available_budget_tag else None\n",
    "    )\n",
    "    \n",
    "    # Extract table headers\n",
    "    headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')]\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = []\n",
    "    for tr in table.find('tbody').find_all('tr'):\n",
    "        # Extract row cells\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "        \n",
    "        # Check the class of the <tr> tag for \"success\" or \"discarded\"\n",
    "        approved = 1 if 'success' in tr.get('class', []) else 0\n",
    "        \n",
    "        # Append cells and approval status\n",
    "        rows.append(cells + [approved])\n",
    "    \n",
    "    # Add \"Approved\" column to the headers\n",
    "    headers.append('Approved')\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    df['Year'] = year  # Add a 'Year' column\n",
    "    df['Available Budget'] = available_budget  # Add the total budget for the year to every row\n",
    "    return df\n",
    "\n",
    "# Scrape and clean tables for all years\n",
    "budget_dataframes = [\n",
    "    scrape_and_clean_budget_table(url, year) for year, url in budget_urls.items()\n",
    "]\n",
    "\n",
    "# Combine all dataframes into one\n",
    "budget_jena_df = pd.concat(budget_dataframes, ignore_index=True)\n",
    "\n",
    "# Clean and transform the DataFrame\n",
    "budget_jena_df['Preis'] = budget_jena_df['Preis'].str.extract(r'(\\d+)').astype(float) * 1000\n",
    "budget_jena_df['Stimmen'] = budget_jena_df['Stimmen'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# Rename columns to English\n",
    "budget_jena_df.rename(columns={\n",
    "    'Vorschlag Titel': 'Proposal Title',\n",
    "    'Stimmen': 'Votes',\n",
    "    'Preis': 'Price',\n",
    "    'Year': 'Year',\n",
    "    'Available Budget': 'Budget for this year',\n",
    "    'Approved': 'Approved'\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns if any remain\n",
    "budget_jena_df = budget_jena_df.loc[:, ~budget_jena_df.columns.str.contains('VerfügbareBudgetmittel', na=False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Comments from Jena Projects (could be probably scaled for other similar cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Updated function to extract comments from a single page\n",
    "def extract_comments_from_page(soup):\n",
    "    comments_data = []\n",
    "    comments_section = soup.find_all('div', class_='comment small-12')\n",
    "    \n",
    "    for comment in comments_section:\n",
    "        # Extract comment text\n",
    "        comment_text = comment.find('p').get_text(strip=True) if comment.find('p') else None\n",
    "        \n",
    "        # Extract username\n",
    "        username_tag = comment.find('span', class_='user-name')\n",
    "        username = username_tag.get_text(strip=True) if username_tag else None\n",
    "        \n",
    "        # Extract date\n",
    "        date_tag = comment.find('div', class_='comment-info').find_all('a')[-1]\n",
    "        date = date_tag.get_text(strip=True) if date_tag else None\n",
    "\n",
    "        \n",
    "        # Extract likes and dislikes (clean and convert to integer)\n",
    "        likes_tag = comment.find('span', class_='in-favor')\n",
    "        likes = int(re.sub(r'\\D', '', likes_tag.get_text(strip=True))) if likes_tag else 0\n",
    "        \n",
    "        dislikes_tag = comment.find('span', class_='against')\n",
    "        dislikes = int(re.sub(r'\\D', '', dislikes_tag.get_text(strip=True))) if dislikes_tag else 0\n",
    "        \n",
    "        # Extract total votes (clean and convert to integer)\n",
    "        total_votes = likes + dislikes\n",
    "        \n",
    "        comments_data.append({\n",
    "            'Text': comment_text,\n",
    "            'Username': username,\n",
    "            'Date': date,\n",
    "            'Likes': likes,\n",
    "            'Dislikes': dislikes,\n",
    "            'Total Votes': total_votes\n",
    "        })\n",
    "    return comments_data\n",
    "\n",
    "\n",
    "# Scrape all comments across pages (pagination logic remains the same)\n",
    "def scrape_all_comments(base_url):\n",
    "    comments = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        paginated_url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        response = requests.get(paginated_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load page {page} for URL: {base_url}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        comments_on_page = extract_comments_from_page(soup)\n",
    "        \n",
    "        if not comments_on_page:  # Stop if no comments on the page\n",
    "            break\n",
    "        \n",
    "        comments.extend(comments_on_page)\n",
    "        page += 1\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Function to scrape the main content and comments for each URL\n",
    "def scrape_content_and_comments(urls):\n",
    "    data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load URL: {url}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Scrape main content\n",
    "        title = soup.find('title').get_text(strip=True) if soup.find('title') else None\n",
    "        content_div = soup.find('div', class_='flex-layout')\n",
    "        content = content_div.get_text(strip=True) if content_div else None\n",
    "        \n",
    "        # Scrape comments\n",
    "        comments = scrape_all_comments(url)\n",
    "        \n",
    "        data.append({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Content': content,\n",
    "            'Comments': comments\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Scrape comments for all URLs\n",
    "urls = df_jena['URL'].tolist()\n",
    "scraped_data = scrape_content_and_comments(urls)\n",
    "\n",
    "# Create structured DataFrame for comments\n",
    "comments_data = []\n",
    "for item in scraped_data:\n",
    "    for comment in item['Comments']:\n",
    "        comment['URL'] = item['URL']  # Link comment to the project URL\n",
    "        comments_data.append(comment)\n",
    "\n",
    "# Create the comments DataFrame\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Create a mapping from URL to Title\n",
    "url_to_title = df_jena_cleaned.set_index('URL')['Title'].to_dict()\n",
    "\n",
    "# Add a 'Project' column to df_comments using the mapping\n",
    "df_comments['Project'] = df_comments['URL'].map(url_to_title)\n",
    "df_comments = df_comments[['URL', 'Project'] + [col for col in df_comments.columns if col not in ['URL', 'Project']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
