{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def content_scraper(soup, links, identifier):\n",
    "    link_and_data = {}\n",
    "\n",
    "    for link in links:  # Iterate over all links\n",
    "        response = requests.get(link)  # Use the current link\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Handle scraping logic based on the identifier\n",
    "            match identifier:\n",
    "                case 'jena':\n",
    "                    title = soup.find_all('title')\n",
    "                    content = soup.find_all('div', class_='flex-layout')\n",
    "\n",
    "                    # Extract title and content if found\n",
    "                    text_content = (\n",
    "                        title[0].get_text(strip=True) + ': ' + content[0].get_text(strip=True)\n",
    "                        if title and content else \"No content found\"\n",
    "                    )\n",
    "                case 'siegburg':\n",
    "                    divs = soup.find_all('div', class_='flex-layout')\n",
    "                    content = divs[1] if len(divs) > 1 else None\n",
    "\n",
    "                    # Extract content if found\n",
    "                    text_content = content.get_text(strip=True) if content else \"No content found\"\n",
    "                case _:\n",
    "                    text_content = \"Wrong identifier\"\n",
    "\n",
    "            link_and_data[link] = text_content\n",
    "        else:\n",
    "            # Handle failed requests\n",
    "            link_and_data[link] = f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "    return link_and_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siegburg_data(soup):\n",
    "    links = []\n",
    "\n",
    "    a_tags = soup.find_all('a')\n",
    "\n",
    "    for tag in a_tags:\n",
    "        href = tag.get('href')\n",
    "        \n",
    "        if href and '/proposals/' in href and not 'new' in href:\n",
    "            link = 'https://mitmachen.siegburg.de' + href\n",
    "    \n",
    "            if not link in links:\n",
    "                links.append(link)\n",
    "    \n",
    "    link_and_content = content_scraper(soup, links, 'siegburg')\n",
    "\n",
    "    return link_and_content\n",
    "\n",
    "\n",
    "def jena_data(soup):\n",
    "    links = []\n",
    "\n",
    "    a_tags = soup.find_all('a',  class_='resource-item--title')\n",
    "\n",
    "    for tag in a_tags:\n",
    "        href = tag.get('href')\n",
    "        \n",
    "        if href:\n",
    "            link = 'https://mitmachen.jena.de' + href\n",
    "            \n",
    "            if not link in links:\n",
    "                links.append(link)\n",
    "    \n",
    "    link_and_content = content_scraper(soup, links, 'jena')\n",
    "\n",
    "    return link_and_content\n",
    "\n",
    "\n",
    "def wurzburg_data(soup):\n",
    "    links = []\n",
    "\n",
    "    # Find all resource links for Würzburg (similar to Jena)\n",
    "    a_tags = soup.find_all('a', class_='resource-item--title')\n",
    "\n",
    "    for tag in a_tags:\n",
    "        href = tag.get('href')\n",
    "\n",
    "        if href:\n",
    "            link = 'https://wuerzburg-mitmachen.de' + href\n",
    "\n",
    "            if link not in links:\n",
    "                links.append(link)\n",
    "\n",
    "    # Scrape content from each link\n",
    "    link_and_content = content_scraper(soup, links, 'jena')  # Reusing 'jena' scraping logic as the structure is the same\n",
    "\n",
    "    return link_and_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://mitmachen.siegburg.de/angebotslandkarte',\n",
    "    'https://mitmachen.jena.de/projekts',\n",
    "    'https://wuerzburg-mitmachen.de/projekts'\n",
    "]\n",
    "\n",
    "\n",
    "def def_42(urls):\n",
    "    urls_and_data = {}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        link_and_content = scraper(soup, url)\n",
    "        urls_and_data[url] = link_and_content\n",
    "\n",
    "    return urls_and_data\n",
    "\n",
    "\n",
    "def scraper(soup, url):\n",
    "    if 'siegburg' in url:\n",
    "        return siegburg_data(soup)\n",
    "    elif 'jena' in url:\n",
    "        return jena_data(soup)\n",
    "    elif 'wuerzburg' in url:  # Add handling for Würzburg\n",
    "        return wurzburg_data(soup)\n",
    "\n",
    "    return \"No scraper defined for this URL\"\n",
    "\n",
    "\n",
    "link_and_data = def_42(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate scraper for 19 similar websites (when you have to check \"inner\" proposals inside of a project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Updated function to extract proposals from a project page\n",
    "def extract_proposals(soup, base_url):\n",
    "    proposals = []\n",
    "    proposal_items = soup.find_all('div', class_='resource-item proposal-list-item')\n",
    "\n",
    "    for proposal in proposal_items:\n",
    "        # Extract title\n",
    "        title_tag = proposal.find('a', class_='resource-item--title')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "        # Extract URL\n",
    "        url = base_url + title_tag['href'] if title_tag and 'href' in title_tag.attrs else None\n",
    "\n",
    "        # Extract description\n",
    "        description_tag = proposal.find('div', class_='resource-item--description')\n",
    "        description = description_tag.get_text(strip=True) if description_tag else None\n",
    "\n",
    "        # Extract author/username\n",
    "        author_tag = proposal.find('a', class_='resource-item--author')\n",
    "        author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "        # Extract number of comments\n",
    "        comments_tag = proposal.find('span', class_='comments')\n",
    "        comments = int(comments_tag.get_text(strip=True).split()[0]) if comments_tag else 0\n",
    "\n",
    "        # Extract number of supporters\n",
    "        supporters_tag = proposal.find('span', class_='total-supports')\n",
    "        supporters = int(supporters_tag.get_text(strip=True).split()[0]) if supporters_tag else 0\n",
    "\n",
    "        # Extract parent project\n",
    "        project_tag = proposal.find('a', class_='breadcrumbs-item')\n",
    "        proposed_for_project = project_tag.get_text(strip=True) if project_tag else None\n",
    "\n",
    "        proposals.append({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Proposed for Project': proposed_for_project,\n",
    "            'Description': description,\n",
    "            'Author': author,\n",
    "            'Comments': comments,\n",
    "            'Supporters': supporters,\n",
    "        })\n",
    "    return proposals\n",
    "\n",
    "\n",
    "# Function to extract comments from a project page\n",
    "def extract_comments(soup, base_url, project_title, city):\n",
    "    comments = []\n",
    "    comment_items = soup.find_all('div', class_='comment small-12')\n",
    "\n",
    "    for comment in comment_items:\n",
    "        # Extract comment text\n",
    "        comment_body = comment.find('div', class_='comment-body')\n",
    "        text = comment_body.get_text(strip=True) if comment_body else None\n",
    "\n",
    "        # Extract author/username\n",
    "        author_tag = comment.find('span', class_='user-name')\n",
    "        author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "        likes_tag = comment.find('span', class_='in-favor')\n",
    "        likes_text = likes_tag.get_text(strip=True) if likes_tag else \"0\"\n",
    "        likes = int(re.sub(r'\\D', '', likes_text)) if likes_text else 0  # Remove non-digit characters\n",
    "\n",
    "        # Extract dislikes\n",
    "        dislikes_tag = comment.find('span', class_='against')\n",
    "        dislikes_text = dislikes_tag.get_text(strip=True) if dislikes_tag else \"0\"\n",
    "        dislikes = int(re.sub(r'\\D', '', dislikes_text)) if dislikes_text else 0  \n",
    "\n",
    "        # Add comment data\n",
    "        comments.append({\n",
    "            'URL': base_url,\n",
    "            'Commented under Project': project_title,\n",
    "            'Text': text,\n",
    "            'Author': author,\n",
    "            'Likes': likes,\n",
    "            'Dislikes': dislikes,\n",
    "            'City': city\n",
    "        })\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "# Function to extract city name from the base URL\n",
    "def extract_city_name(base_url):\n",
    "    # Words to remove from the city name\n",
    "    remove_words = ['mitmachen', 'Mitmachen', 'mitwirken', 'Smarte', 'region', 'unser', 'mitgestalten', 'gestalten', 'machmit', 'dialog', 'consul', 'www', 'de', 'https', 'com']\n",
    "\n",
    "    # Split the URL into parts (by '.' or '/')\n",
    "    parts = base_url.replace('https://', '').replace('http://', '').split('.')\n",
    "    all_parts = [part.split('/')[0] for part in parts]  # Handle cases where \"/\" exists after domain\n",
    "\n",
    "    # Remove known unwanted words and empty strings\n",
    "    filtered_parts = [part for part in all_parts if part.lower() not in remove_words and part]\n",
    "\n",
    "    # Return the first relevant part (assumes city name is left after filtering)\n",
    "    city = filtered_parts[0].replace('-', ' ').capitalize() if filtered_parts else \"Unknown\"\n",
    "\n",
    "    # Remove unwanted words from city name\n",
    "    for word in remove_words:\n",
    "        city = city.replace(word, '')\n",
    "    \n",
    "\n",
    "    return city.strip().capitalize()\n",
    "\n",
    "\n",
    "# Update project scraping to include comments\n",
    "def scrape_project_page_with_comments(url, base_url, city):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load project page: {url}\")\n",
    "        return None, [], []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract project title\n",
    "    title_tag = soup.find('title')\n",
    "    project_title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "    # Extract project description\n",
    "    content_div = soup.find('div', class_='flex-layout')\n",
    "    description = content_div.get_text(strip=True) if content_div else None\n",
    "\n",
    "    # Extract comments\n",
    "    comments = extract_comments(soup, base_url, project_title, city)\n",
    "\n",
    "    # Extract proposals\n",
    "    proposals = extract_proposals(soup, base_url=base_url)\n",
    "\n",
    "    return {\n",
    "        'Project URL': url,\n",
    "        'Project Title': project_title,\n",
    "        'Project Description': description,\n",
    "        'Proposal Count': len(proposals),\n",
    "    }, proposals, comments\n",
    "\n",
    "\n",
    "# Main function to scrape projects with comments\n",
    "def scrape_projects_with_comments(main_url, base_url):\n",
    "    response = requests.get(main_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load main projects page: {main_url}\")\n",
    "        return None, None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all project links\n",
    "    links = soup.find_all('a', class_='resource-item--title')\n",
    "    project_links = [base_url + link['href'] for link in links if 'href' in link.attrs]\n",
    "\n",
    "    projects = []\n",
    "    all_proposals = []\n",
    "    all_comments = []\n",
    "\n",
    "    for project_url in project_links:\n",
    "        project_data, proposals, comments = scrape_project_page_with_comments(project_url, base_url, extract_city_name(base_url))\n",
    "        if project_data:\n",
    "            projects.append(project_data)\n",
    "            all_proposals.extend(proposals)\n",
    "            all_comments.extend(comments)\n",
    "\n",
    "    return pd.DataFrame(projects), pd.DataFrame(all_proposals), pd.DataFrame(all_comments)\n",
    "\n",
    "# List of websites (fixed flensburg-mitmachen.de base_url)\n",
    "websites = [\n",
    "    {\"main_url\": \"https://wuerzburg-mitmachen.de/projekts\", \"base_url\": \"https://wuerzburg-mitmachen.de\"},\n",
    "    {\"main_url\": \"https://mitmachen.siegburg.de/projekts\", \"base_url\": \"https://mitmachen.siegburg.de\"}, \n",
    "    {\"main_url\": \"https://mitmachen.jena.de/projekts\", \"base_url\": \"https://mitmachen.jena.de\"},\n",
    "    {\"main_url\": \"https://mitmachgemeinde.de/projekts\", \"base_url\": \"https://mitmachgemeinde.de\"},\n",
    "    {\"main_url\": \"https://bamberg-gestalten.de/projekts\", \"base_url\": \"https://bamberg-gestalten.de\"},\n",
    "    {\"main_url\": \"https://mitmachen-pforzheim.de/projekts\", \"base_url\": \"https://mitmachen-pforzheim.de\"},\n",
    "    # {\"main_url\": \"https://bochum-mitgestalten.de/projekts\", \"base_url\": \"https://bochum-mitgestalten.de\"},\n",
    "    # {\"main_url\": \"https://unser.muenchen.de/projekts\", \"base_url\": \"https://unser.muenchen.de\"},\n",
    "    # {\"main_url\": \"https://mitreden.ilzerland.bayern/projekts\", \"base_url\": \"https://mitreden.ilzerland.bayern\"},\n",
    "    # {\"main_url\": \"https://stutensee-mitwirken.de/projekts\", \"base_url\": \"https://stutensee-mitwirken.de\"},\n",
    "    # {\"main_url\": \"https://consul.unterschleissheim.de/projekts\", \"base_url\": \"https://consul.unterschleissheim.de\"},\n",
    "    # {\"main_url\": \"https://machmit.kempten.de/projekts\", \"base_url\": \"https://machmit.kempten.de\"},\n",
    "    # {\"main_url\": \"https://consul.detmold-mitgestalten.de/projekts\", \"base_url\": \"https://consul.detmold-mitgestalten.de\"},\n",
    "    # {\"main_url\": \"https://flensburg-mitmachen.de/projekts\", \"base_url\": \"https://flensburg-mitmachen.de\"},  # Fixed URL\n",
    "    # {\"main_url\": \"https://mitmachen.amberg.de/projekts\", \"base_url\": \"https://mitmachen.amberg.de\"},\n",
    "    # {\"main_url\": \"https://mitmachen.smarte-region-linz.de/projekts\", \"base_url\": \"https://mitmachen.smarte-region-linz.de\"},\n",
    "    # {\"main_url\": \"https://mitgestalten.trier.de/projekts\", \"base_url\": \"https://mitgestalten.trier.de\"},\n",
    "    # {\"main_url\": \"https://machmit.augsburg.de/projekts\", \"base_url\": \"https://machmit.augsburg.de\"},\n",
    "]\n",
    "\n",
    "# Initialize empty dataframes for all projects and proposals\n",
    "all_projects_df = pd.DataFrame()\n",
    "all_proposals_df = pd.DataFrame()\n",
    "all_comments_df = pd.DataFrame()\n",
    "\n",
    "# Scrape all websites and merge results\n",
    "for site in websites:\n",
    "    main_url = site[\"main_url\"]\n",
    "    base_url = site[\"base_url\"]\n",
    "\n",
    "    # Extract city name using the new function\n",
    "    city = extract_city_name(base_url)\n",
    "\n",
    "    try:\n",
    "        # Scrape projects, proposals, and comments\n",
    "        projects_df, proposals_df, comments_df = scrape_projects_with_comments(main_url, base_url)\n",
    "\n",
    "        # Add a 'City' column to all DataFrames\n",
    "        projects_df['City'] = city\n",
    "        proposals_df['City'] = city\n",
    "        comments_df['City'] = city\n",
    "\n",
    "        # Append results to the combined DataFrames\n",
    "        all_projects_df = pd.concat([all_projects_df, projects_df], ignore_index=True)\n",
    "        all_proposals_df = pd.concat([all_proposals_df, proposals_df], ignore_index=True)\n",
    "        all_comments_df = pd.concat([all_comments_df, comments_df], ignore_index=True)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to scrape {main_url} - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City\n",
       "Siegburg     96\n",
       "Wuerzburg    39\n",
       "Pforzheim    34\n",
       "Bamberg      32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_proposals_df.value_counts('City')\n",
    "\n",
    "# Make one big dataset from this scraper (add Jena [Project Start Date, Comments amount, Supporters, Discussion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jena_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Separate URLs and content for Siegburg and Jena\n",
    "siegburg_data = {k: v for k, v in link_and_data.items() if \"siegburg\" in k}\n",
    "jena_data = {k: v for k, v in link_and_data.items() if \"jena\" in k}\n",
    "# wurzburg_data = {k: v for k, v in link_and_data.items() if \"wuerzburg\" in k}\n",
    "\n",
    "# Create DataFrames\n",
    "df_sieburg = pd.DataFrame(list(siegburg_data.items()), columns=[\"URL\", \"Content\"])\n",
    "df_jena = pd.DataFrame(list(jena_data.items()), columns=[\"URL\", \"Content\"])\n",
    "# df_wurzburg = pd.DataFrame(list(wurzburg_data.items()), columns=[\"URL\", \"Content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jena = pd.DataFrame(df_jena['Content'][0].items(), columns=[\"URL\", \"Content\"])\n",
    "df_sieburg = pd.DataFrame(df_sieburg['Content'][0].items(), columns=[\"URL\", \"Content\"])\n",
    "# df_wurzburg_projects, df_wurzburg_proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and structurising Sieburg Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Enhanced function to extract all logical parts, including \"Unterstützer*innen\"\n",
    "def extract_full_data_with_supporters(content):\n",
    "    # Extract title (everything before the first date)\n",
    "    title_match = re.search(r'^(.*?)(\\r|\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "    title = title_match.group(1).strip() if title_match else None\n",
    "\n",
    "    # Extract date\n",
    "    date_match = re.search(r'\\d{1,2}\\.\\s\\w+\\s\\d{4}', content)\n",
    "    date = date_match.group(0) if date_match else None\n",
    "\n",
    "    # Extract comments count\n",
    "    comments_match = re.search(r'(\\d+)\\sKommentare', content)\n",
    "    comments = int(comments_match.group(1)) if comments_match else 0\n",
    "\n",
    "    # Extract tags (sections with numbers or + signs)\n",
    "    tags_match = re.findall(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "    tags = ', '.join(tags_match) if tags_match else None\n",
    "\n",
    "    # Extract description (everything after \"Geselliges Beisammensein\" or similar patterns)\n",
    "    description_start = re.search(r'(Geselliges Beisammensein|Angebotslandkarte)', content)\n",
    "    description = content[description_start.start():].strip() if description_start else None\n",
    "\n",
    "    # Extract username\n",
    "    username_match = re.search(r'(\\w+\\s\\w+|Beigetreten am:.*?\\d{4})', content)\n",
    "    username = username_match.group(1).split('Beigetreten am:')[0].strip() if username_match else None\n",
    "\n",
    "    # Extract Vorschläge count\n",
    "    vorschlaege_match = re.search(r'Vorschläge(\\d+)', content)\n",
    "    vorschlaege = int(vorschlaege_match.group(1)) if vorschlaege_match else 0\n",
    "\n",
    "    # Extract Konto verification status\n",
    "    konto_match = re.search(r'(Konto\\s(verifiziert|ist nicht verifiziert))', content)\n",
    "    konto_status = konto_match.group(2) if konto_match else None\n",
    "\n",
    "    # # Extract registration date\n",
    "    # registration_match = re.search(r'Beigetreten am:\\s(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "    # registration_date = registration_match.group(1) if registration_match else None\n",
    "\n",
    "    # Extract number of Unterstützer*innen\n",
    "    supporters_match = re.search(r'(\\d+)\\sUnterstützer\\*in', content)\n",
    "    supporters = int(supporters_match.group(1)) if supporters_match else 0\n",
    "\n",
    "    return title, date, comments, tags, description, username, vorschlaege, konto_status, supporters\n",
    "\n",
    "# Apply the enhanced function to the DataFrame and create new columns\n",
    "df_sieburg[['Title', 'Date', 'Comments', 'Tags', 'Description', 'Username', 'Vorschläge', 'Konto Status', 'Supporters']] = df_sieburg['Content'].apply(\n",
    "    lambda x: pd.Series(extract_full_data_with_supporters(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean description considering keywords, numeric patterns, and refined starting logic\n",
    "def clean_description_advanced(content):\n",
    "    # Define keywords that mark the beginning of the description\n",
    "    keywords = [\n",
    "        'Geselliges Beisammensein', 'Natur', 'Hilfe & Beratung', 'Bildung', \n",
    "        'Musik', 'Bewegung', 'Glaube', 'Kulinarisches', 'Kunst & Kultur', 'Sonstiges',\n",
    "    ]\n",
    "    \n",
    "    # Check for keywords first\n",
    "    for keyword in keywords:\n",
    "        if keyword in content:\n",
    "            start_idx = content.find(keyword) + len(keyword)\n",
    "            description = content[start_idx:].strip()\n",
    "            description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "            return description\n",
    "\n",
    "    # If no keyword is found, check for numeric patterns like \"18-24, 25-49, etc.\"\n",
    "    numeric_pattern = re.search(r'(\\d{1,2}[-+]\\d{1,2}|\\d{2}\\+)', content)\n",
    "    if numeric_pattern:\n",
    "        start_idx = numeric_pattern.end()\n",
    "        description = content[start_idx:].strip()\n",
    "        description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "        return description\n",
    "\n",
    "    # As a fallback, find the first capital letter, quote, or digit to mark the start\n",
    "    fallback_match = re.search(r'[A-Z\"0-9]', content)\n",
    "    if fallback_match:\n",
    "        start_idx = fallback_match.start()\n",
    "        description = content[start_idx:].strip()\n",
    "        description = re.split(r'(Kommentare\\(.*?\\)|registrieren)', description)[0].strip()\n",
    "        return description\n",
    "\n",
    "    # If nothing works, return the content as is\n",
    "    return content\n",
    "\n",
    "# Apply the advanced cleaning function to the Description column\n",
    "df_sieburg['Description'] = df_sieburg['Content'].apply(clean_description_advanced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and structurising Jena Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized function to process all entries in df_jena['Content']\n",
    "def process_all_jena_entries(df_jena):\n",
    "    # Generalized function for extracting data\n",
    "    def extract_jena_data(content, soup):\n",
    "        # Extract Title\n",
    "        title_match = re.search(r'^(.*?):', content)\n",
    "        title = title_match.group(1).strip() if title_match else None\n",
    "\n",
    "        # Extract Description\n",
    "        description_match = re.search(r':\\s*(.*?)\\n⭐', content, re.DOTALL)\n",
    "        description = description_match.group(1).strip() if description_match else None\n",
    "\n",
    "        # Extract Project Start Date\n",
    "        start_date_match = re.search(r'Projektstart\\s*(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "        start_date = start_date_match.group(1) if start_date_match else None\n",
    "\n",
    "        # Extract Username\n",
    "        username_match = re.search(r'@(\\w+)', content)\n",
    "        username = username_match.group(1) if username_match else None\n",
    "\n",
    "        # Extract Tags\n",
    "        tags_match = re.findall(r'#(\\w+)', content)\n",
    "        tags = ', '.join(tags_match) if tags_match else None\n",
    "\n",
    "        # Extract Supporters Count\n",
    "        supporters_match = re.search(r'(\\d+)\\sUnterstützer\\*in', content)\n",
    "        supporters = int(supporters_match.group(1)) if supporters_match else None\n",
    "\n",
    "        # Extract Number of Discussions\n",
    "        discussions_match = re.search(r'DiskussionenAbgeschlossen\\sam\\s(\\d{1,2}\\.\\s\\w+\\s\\d{4})', content)\n",
    "        discussions = discussions_match.group(1) if discussions_match else None\n",
    "\n",
    "        # Extract Number of Comments from the h4 tag\n",
    "        comments_tag = soup.find('h4', text=re.compile(r'Kommentare'))\n",
    "        comments_count = int(re.search(r'\\((\\d+)\\)', comments_tag.get_text(strip=True)).group(1)) if comments_tag else 0\n",
    "\n",
    "        return {\n",
    "            \"Title\": title,\n",
    "            \"Description\": description,\n",
    "            \"Project Start Date\": start_date,\n",
    "            \"Username\": username,\n",
    "            \"Tags\": tags,\n",
    "            \"Supporters\": supporters,\n",
    "            \"Discussions\": discussions,\n",
    "            \"Comments\": comments_count\n",
    "        }\n",
    "\n",
    "\n",
    "    extracted_data = []\n",
    "    for _, row in df_jena.iterrows():\n",
    "        response = requests.get(row[\"URL\"])\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load URL: {row['URL']}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        data = extract_jena_data(row[\"Content\"], soup)\n",
    "        extracted_data.append(data)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df_jena_cleaned = pd.DataFrame([{\n",
    "        \"URL\": row[\"URL\"],\n",
    "        \"Title\": data[\"Title\"],\n",
    "        # \"Description\": data[\"Description\"],\n",
    "        \"Project Start Date\": data[\"Project Start Date\"],\n",
    "        # \"Username\": data[\"Username\"],\n",
    "        # \"Tags\": data[\"Tags\"],\n",
    "        \"Supporters\": data[\"Supporters\"],\n",
    "        \"Discussions\": data[\"Discussions\"],\n",
    "        \"Comments\": data[\"Comments\"]\n",
    "    } for row, data in zip(df_jena.to_dict('records'), extracted_data)])\n",
    "\n",
    "    return df_jena_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rusel\\AppData\\Local\\Temp\\ipykernel_18664\\120626955.py:34: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  comments_tag = soup.find('h4', text=re.compile(r'Kommentare'))\n"
     ]
    }
   ],
   "source": [
    "df_jena_cleaned = process_all_jena_entries(df_jena)\n",
    "# df_wurzburg_cleaned = process_all_jena_entries(df_wurzburg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_wurzburg_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_wurzburg_cleaned\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_wurzburg_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# df_wurzburg_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BurgerBudgets in Jena (2024, 23, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for the budgets\n",
    "budget_urls = {\n",
    "    2024: \"https://mitmachen.jena.de/buergerbudget\",\n",
    "    2023: \"https://mitmachen.jena.de/buergerbudget-2023\",\n",
    "    2022: \"https://mitmachen.jena.de/buergerbudget-2022\"\n",
    "}\n",
    "\n",
    "# Updated function to scrape and clean a budget table for a given year\n",
    "def scrape_and_clean_budget_table(url, year):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load URL: {url}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', id='budget-investments-compatible')  # Locate the table by its ID\n",
    "    \n",
    "    if not table:\n",
    "        print(f\"No table found for URL: {url}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the total available budget for the year (last <th> in <thead>)\n",
    "    available_budget_tag = table.find('thead').find_all('th')[-1]  # Find the last <th>\n",
    "    available_budget = (\n",
    "        float(re.sub(r'[^\\d.]', '', available_budget_tag.get_text(strip=True))) * 1000\n",
    "        if available_budget_tag else None\n",
    "    )\n",
    "    \n",
    "    # Extract table headers\n",
    "    headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')]\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = []\n",
    "    for tr in table.find('tbody').find_all('tr'):\n",
    "        # Extract row cells\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "        \n",
    "        # Check the class of the <tr> tag for \"success\" or \"discarded\"\n",
    "        approved = 1 if 'success' in tr.get('class', []) else 0\n",
    "        \n",
    "        # Append cells and approval status\n",
    "        rows.append(cells + [approved])\n",
    "    \n",
    "    # Add \"Approved\" column to the headers\n",
    "    headers.append('Approved')\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    df['Year'] = year  # Add a 'Year' column\n",
    "    df['Available Budget'] = available_budget  # Add the total budget for the year to every row\n",
    "    return df\n",
    "\n",
    "# Scrape and clean tables for all years\n",
    "budget_dataframes = [\n",
    "    scrape_and_clean_budget_table(url, year) for year, url in budget_urls.items()\n",
    "]\n",
    "\n",
    "# Combine all dataframes into one\n",
    "budget_jena_df = pd.concat(budget_dataframes, ignore_index=True)\n",
    "\n",
    "# Clean and transform the DataFrame\n",
    "budget_jena_df['Preis'] = budget_jena_df['Preis'].str.extract(r'(\\d+)').astype(float) * 1000\n",
    "budget_jena_df['Stimmen'] = budget_jena_df['Stimmen'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# Rename columns to English\n",
    "budget_jena_df.rename(columns={\n",
    "    'Vorschlag Titel': 'Proposal Title',\n",
    "    'Stimmen': 'Votes',\n",
    "    'Preis': 'Price',\n",
    "    'Year': 'Year',\n",
    "    'Available Budget': 'Budget for this year',\n",
    "    'Approved': 'Approved'\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns if any remain\n",
    "budget_jena_df = budget_jena_df.loc[:, ~budget_jena_df.columns.str.contains('VerfügbareBudgetmittel', na=False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Comments from Jena Projects (could be probably scaled for other similar cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Updated function to extract comments from a single page\n",
    "def extract_comments_from_page(soup):\n",
    "    comments_data = []\n",
    "    comments_section = soup.find_all('div', class_='comment small-12')\n",
    "    \n",
    "    for comment in comments_section:\n",
    "        # Extract comment text\n",
    "        comment_text = comment.find('p').get_text(strip=True) if comment.find('p') else None\n",
    "        \n",
    "        # Extract username\n",
    "        username_tag = comment.find('span', class_='user-name')\n",
    "        username = username_tag.get_text(strip=True) if username_tag else None\n",
    "        \n",
    "        # Extract date\n",
    "        date_tag = comment.find('div', class_='comment-info').find_all('a')[-1]\n",
    "        date = date_tag.get_text(strip=True) if date_tag else None\n",
    "\n",
    "        \n",
    "        # Extract likes and dislikes (clean and convert to integer)\n",
    "        likes_tag = comment.find('span', class_='in-favor')\n",
    "        likes = int(re.sub(r'\\D', '', likes_tag.get_text(strip=True))) if likes_tag else 0\n",
    "        \n",
    "        dislikes_tag = comment.find('span', class_='against')\n",
    "        dislikes = int(re.sub(r'\\D', '', dislikes_tag.get_text(strip=True))) if dislikes_tag else 0\n",
    "        \n",
    "        # Extract total votes (clean and convert to integer)\n",
    "        total_votes = likes + dislikes\n",
    "        \n",
    "        comments_data.append({\n",
    "            'Text': comment_text,\n",
    "            'Username': username,\n",
    "            'Date': date,\n",
    "            'Likes': likes,\n",
    "            'Dislikes': dislikes,\n",
    "            'Total Votes': total_votes\n",
    "        })\n",
    "    return comments_data\n",
    "\n",
    "\n",
    "# Scrape all comments across pages (pagination logic remains the same)\n",
    "def scrape_all_comments(base_url):\n",
    "    comments = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        paginated_url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        response = requests.get(paginated_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load page {page} for URL: {base_url}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        comments_on_page = extract_comments_from_page(soup)\n",
    "        \n",
    "        if not comments_on_page:  # Stop if no comments on the page\n",
    "            break\n",
    "        \n",
    "        comments.extend(comments_on_page)\n",
    "        page += 1\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Function to scrape the main content and comments for each URL\n",
    "def scrape_content_and_comments(urls):\n",
    "    data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to load URL: {url}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Scrape main content\n",
    "        title = soup.find('title').get_text(strip=True) if soup.find('title') else None\n",
    "        content_div = soup.find('div', class_='flex-layout')\n",
    "        content = content_div.get_text(strip=True) if content_div else None\n",
    "        \n",
    "        # Scrape comments\n",
    "        comments = scrape_all_comments(url)\n",
    "        \n",
    "        data.append({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Content': content,\n",
    "            'Comments': comments\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Scrape comments for all URLs\n",
    "urls = df_jena['URL'].tolist()\n",
    "scraped_data = scrape_content_and_comments(urls)\n",
    "\n",
    "# Create structured DataFrame for comments\n",
    "comments_data = []\n",
    "for item in scraped_data:\n",
    "    for comment in item['Comments']:\n",
    "        comment['URL'] = item['URL']  # Link comment to the project URL\n",
    "        comments_data.append(comment)\n",
    "\n",
    "# Create the comments DataFrame\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Create a mapping from URL to Title\n",
    "url_to_title = df_jena_cleaned.set_index('URL')['Title'].to_dict()\n",
    "\n",
    "# Add a 'Project' column to df_comments using the mapping\n",
    "df_comments['Project'] = df_comments['URL'].map(url_to_title)\n",
    "df_comments = df_comments[['URL', 'Project'] + [col for col in df_comments.columns if col not in ['URL', 'Project']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Project</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Dislikes</th>\n",
       "      <th>Total Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://mitmachen.jena.de/stufe-1-kurzfristige...</td>\n",
       "      <td>Stufe I - Kurzfristige Entwickelbarkeit</td>\n",
       "      <td>Ich kann nicht ganz nachvollziehen, warum in d...</td>\n",
       "      <td>r_luen</td>\n",
       "      <td>05. November 2024 17:16:56</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://mitmachen.jena.de/stufe-1-kurzfristige...</td>\n",
       "      <td>Stufe I - Kurzfristige Entwickelbarkeit</td>\n",
       "      <td>Den Campusradweg kann man ruhig ins Reich der ...</td>\n",
       "      <td>Klaus.Kleiner</td>\n",
       "      <td>13. November 2024 05:29:18</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://mitmachen.jena.de/stufe-1-kurzfristige...</td>\n",
       "      <td>Stufe I - Kurzfristige Entwickelbarkeit</td>\n",
       "      <td>Stichwort: Straßenbahn!</td>\n",
       "      <td>Klaus.kleiner.77</td>\n",
       "      <td>17. November 2024 09:25:14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://mitmachen.jena.de/stufe-1-kurzfristige...</td>\n",
       "      <td>Stufe I - Kurzfristige Entwickelbarkeit</td>\n",
       "      <td>Bis auf I-B und I-D ist das mehr oder weniger ...</td>\n",
       "      <td>Brabax</td>\n",
       "      <td>08. November 2024 20:08:28</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://mitmachen.jena.de/stufe-1-kurzfristige...</td>\n",
       "      <td>Stufe I - Kurzfristige Entwickelbarkeit</td>\n",
       "      <td>Eine Straßenbahn die völlig unrealistisch ist!...</td>\n",
       "      <td>Klaus.Kleiner</td>\n",
       "      <td>13. November 2024 05:31:55</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>https://mitmachen.jena.de/szenario-3-langfrist...</td>\n",
       "      <td>Szenario 3 „langfristige Flächenverfügbarkeit“</td>\n",
       "      <td>An dem Entwurf gefällt mir im Gegensatz zu den...</td>\n",
       "      <td>Schlo</td>\n",
       "      <td>05. Februar 2024 22:19:32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>https://mitmachen.jena.de/szenario-3-langfrist...</td>\n",
       "      <td>Szenario 3 „langfristige Flächenverfügbarkeit“</td>\n",
       "      <td>Die B&amp;R unter dem  Empfangsgebäude entdeckt?</td>\n",
       "      <td>Touringer</td>\n",
       "      <td>05. Februar 2024 22:45:49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>https://mitmachen.jena.de/szenario-3-langfrist...</td>\n",
       "      <td>Szenario 3 „langfristige Flächenverfügbarkeit“</td>\n",
       "      <td>Szenario 3 hat zwar den Vorteil, die Verknüpfu...</td>\n",
       "      <td>KarSteN</td>\n",
       "      <td>06. Februar 2024 23:32:51</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>https://mitmachen.jena.de/szenario-3-langfrist...</td>\n",
       "      <td>Szenario 3 „langfristige Flächenverfügbarkeit“</td>\n",
       "      <td>Auch die Aufwertung des Bahnhofs kann keine 15...</td>\n",
       "      <td>Bündnis \"Fernverkehr für Jena\"</td>\n",
       "      <td>16. Februar 2024 13:51:18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>https://mitmachen.jena.de/szenario-3-langfrist...</td>\n",
       "      <td>Szenario 3 „langfristige Flächenverfügbarkeit“</td>\n",
       "      <td>Wenn Zeiss so nahe am Westbahnhof baut und lau...</td>\n",
       "      <td>pfingstochse78</td>\n",
       "      <td>07. Februar 2024 09:04:27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  \\\n",
       "0    https://mitmachen.jena.de/stufe-1-kurzfristige...   \n",
       "1    https://mitmachen.jena.de/stufe-1-kurzfristige...   \n",
       "2    https://mitmachen.jena.de/stufe-1-kurzfristige...   \n",
       "3    https://mitmachen.jena.de/stufe-1-kurzfristige...   \n",
       "4    https://mitmachen.jena.de/stufe-1-kurzfristige...   \n",
       "..                                                 ...   \n",
       "309  https://mitmachen.jena.de/szenario-3-langfrist...   \n",
       "310  https://mitmachen.jena.de/szenario-3-langfrist...   \n",
       "311  https://mitmachen.jena.de/szenario-3-langfrist...   \n",
       "312  https://mitmachen.jena.de/szenario-3-langfrist...   \n",
       "313  https://mitmachen.jena.de/szenario-3-langfrist...   \n",
       "\n",
       "                                            Project  \\\n",
       "0           Stufe I - Kurzfristige Entwickelbarkeit   \n",
       "1           Stufe I - Kurzfristige Entwickelbarkeit   \n",
       "2           Stufe I - Kurzfristige Entwickelbarkeit   \n",
       "3           Stufe I - Kurzfristige Entwickelbarkeit   \n",
       "4           Stufe I - Kurzfristige Entwickelbarkeit   \n",
       "..                                              ...   \n",
       "309  Szenario 3 „langfristige Flächenverfügbarkeit“   \n",
       "310  Szenario 3 „langfristige Flächenverfügbarkeit“   \n",
       "311  Szenario 3 „langfristige Flächenverfügbarkeit“   \n",
       "312  Szenario 3 „langfristige Flächenverfügbarkeit“   \n",
       "313  Szenario 3 „langfristige Flächenverfügbarkeit“   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    Ich kann nicht ganz nachvollziehen, warum in d...   \n",
       "1    Den Campusradweg kann man ruhig ins Reich der ...   \n",
       "2                              Stichwort: Straßenbahn!   \n",
       "3    Bis auf I-B und I-D ist das mehr oder weniger ...   \n",
       "4    Eine Straßenbahn die völlig unrealistisch ist!...   \n",
       "..                                                 ...   \n",
       "309  An dem Entwurf gefällt mir im Gegensatz zu den...   \n",
       "310       Die B&R unter dem  Empfangsgebäude entdeckt?   \n",
       "311  Szenario 3 hat zwar den Vorteil, die Verknüpfu...   \n",
       "312  Auch die Aufwertung des Bahnhofs kann keine 15...   \n",
       "313  Wenn Zeiss so nahe am Westbahnhof baut und lau...   \n",
       "\n",
       "                           Username                        Date  Likes  \\\n",
       "0                            r_luen  05. November 2024 17:16:56      9   \n",
       "1                     Klaus.Kleiner  13. November 2024 05:29:18      6   \n",
       "2                  Klaus.kleiner.77  17. November 2024 09:25:14      2   \n",
       "3                            Brabax  08. November 2024 20:08:28      6   \n",
       "4                     Klaus.Kleiner  13. November 2024 05:31:55      5   \n",
       "..                              ...                         ...    ...   \n",
       "309                           Schlo   05. Februar 2024 22:19:32      0   \n",
       "310                       Touringer   05. Februar 2024 22:45:49      0   \n",
       "311                         KarSteN   06. Februar 2024 23:32:51      1   \n",
       "312  Bündnis \"Fernverkehr für Jena\"   16. Februar 2024 13:51:18      2   \n",
       "313                  pfingstochse78   07. Februar 2024 09:04:27      0   \n",
       "\n",
       "     Dislikes  Total Votes  \n",
       "0           0            9  \n",
       "1           2            8  \n",
       "2           1            3  \n",
       "3           0            6  \n",
       "4           4            9  \n",
       "..        ...          ...  \n",
       "309         2            2  \n",
       "310         0            0  \n",
       "311         4            5  \n",
       "312         0            2  \n",
       "313         0            0  \n",
       "\n",
       "[314 rows x 8 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have comment scraper just for Jena and generalized one. Compare them, combine into one. Also compare the amount of comments. Do they scrape equally effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
